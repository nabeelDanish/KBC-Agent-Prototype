{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c277e9",
   "metadata": {},
   "source": [
    "<h1>Fine-tuning DialoGPT on Sentence Generation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f37efd",
   "metadata": {},
   "source": [
    "<h3>Import Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4eee5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2021-12-04 17:11:18.475860: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-04 17:11:18.475900: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "\n",
    "# define a rich console logger\n",
    "console=Console(record=True)\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "cuda.empty_cache()\n",
    "device = 'cpu'\n",
    "\n",
    "import json\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def display_df(df):\n",
    "  \"\"\"display dataframe in ASCII format\"\"\"\n",
    "\n",
    "  console=Console()\n",
    "  table = Table(Column(\"source_text\", justify=\"center\" ), Column(\"target_text\", justify=\"center\"), title=\"Sample Data\",pad_edge=False, box=box.ASCII)\n",
    "\n",
    "  for i, row in enumerate(df.values.tolist()):\n",
    "    table.add_row(row[0], row[1])\n",
    "\n",
    "  console.print(table)\n",
    "\n",
    "training_logger = Table(Column(\"Epoch\", justify=\"center\" ), \n",
    "                        Column(\"Steps\", justify=\"center\"),\n",
    "                        Column(\"Loss\", justify=\"center\"), \n",
    "                        title=\"Training Status\",pad_edge=False, box=box.ASCII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b56a758b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc6bf5",
   "metadata": {},
   "source": [
    "<h3>DataSetClass</h3>\n",
    "<h4>Custom dataset class for loading the dataset and passing it to the model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f99df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and \n",
    "    loading it into the dataloader to pass it to the neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "\n",
    "        #cleaning data so as to ensure data is in string type\n",
    "        source_text = ' '.join(source_text.split())\n",
    "        target_text = ' '.join(target_text.split())\n",
    "\n",
    "        #source = self.tokenizer.batch_encode_plus([source_text], return_tensors='pt')\n",
    "        #target = self.tokenizer.batch_encode_plus([target_text], return_tensors='pt')\n",
    "        \n",
    "        source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        temp = {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6c321",
   "metadata": {},
   "source": [
    "<h3>Train method</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297d5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to be called for training with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, labels=y)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if _%10==0:\n",
    "            training_logger.add_row(str(epoch), str(_), str(loss))\n",
    "            console.print(training_logger)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4600631e",
   "metadata": {},
   "source": [
    "<h3>Validate method</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab6fca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to evaluate model for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    prompts = []\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "      for _, data in enumerate(loader, 0):\n",
    "          y = data['target_ids'].to(device, dtype = torch.long)\n",
    "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "          generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=64, \n",
    "              num_beams=2,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True,\n",
    "              pad_token_id=tokenizer.eos_token_id\n",
    "              )\n",
    "        \n",
    "          prompt = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in ids]\n",
    "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "          if _%10==0:\n",
    "              console.print(f'Completed {_}')\n",
    "\n",
    "          prompts.extend(prompt)\n",
    "          predictions.extend(preds)\n",
    "          actuals.extend(target)\n",
    "            \n",
    "          #print(f\"Predictions: {predictions}\\nActuals: {actuals}\\nPrompt: {prompt}\")\n",
    "    return prompts, predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feb4e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tokenizer, model, source_text_key, target_text_key, source_text, target_text, model_params):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    val_params = {\n",
    "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "      'shuffle': False,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        step = 0\n",
    "        while True:\n",
    "            print(f\"\\nSTEP #{step}\")\n",
    "            if step == 0:\n",
    "                user_input = source_text\n",
    "            else:\n",
    "                user_input = input(\">> user:\") + tokenizer.eos_token\n",
    "            \n",
    "            data_src = pd.DataFrame([{'knowledge_sent': user_input, 'human_sent': target_text}])\n",
    "            data_loader = DataLoader(DataSetClass(data_src, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "                                                      model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text_key, target_text_key), **val_params)\n",
    "            #print(f\"data type: {data.testthis('target')}\")\n",
    "\n",
    "            for _, data in enumerate(data_loader, 0):\n",
    "                #y = data['target_ids'].to(device, dtype = torch.long)\n",
    "                source_ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "                mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "                #print(f\"\\nIDs:\\n{ids}\\n\\nMASKS:\\n{mask}\")\n",
    "\n",
    "                input_ids = torch.cat([past_gen_ids, source_ids], dim=-1) if step > 0 else source_ids\n",
    "                input_ids = input_ids.to(device, dtype = torch.long)\n",
    "                \n",
    "                print(f\"\\nGenerated ids len: {past_gen_ids.shape if step > 0 else source_ids.shape}\\nsource_ids len: {source_ids.shape}\\ninput_ids len: {input_ids.shape}\\nINPUT IDS: {input_ids}\\n\")\n",
    "                \n",
    "                generated_ids = model.generate(\n",
    "                    input_ids = input_ids,\n",
    "                    attention_mask = mask, \n",
    "                    max_length=model_params[\"MAX_SOURCE_TEXT_LENGTH\"] + model_params[\"MAX_TARGET_TEXT_LENGTH\"], \n",
    "                    num_beams=2,\n",
    "                    repetition_penalty=2.5, \n",
    "                    length_penalty=1.0, \n",
    "                    early_stopping=True,\n",
    "                    temperature=5,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    num_return_sequences=1\n",
    "                  ).to(device, dtype = torch.long)\n",
    "\n",
    "                output_list = []\n",
    "                for output in generated_ids:\n",
    "                    output_list.append(\n",
    "                        tokenizer.decode(\n",
    "                            output[input_ids.shape[-1]:], skip_special_tokens=True\n",
    "                        )\n",
    "                     )\n",
    "                    \n",
    "                past_gen_ids = generated_ids[0][input_ids.shape[-1]:].unsqueeze(0)\n",
    "                    \n",
    "                prompt = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in input_ids]\n",
    "                #target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "                #actuals.extend(target)\n",
    "\n",
    "                print(f\"Predictions: {output_list}\\nPrompt: {prompt}\")\n",
    "\n",
    "                #print(\"DialoGPT: {}\".format(tokenizer.decode(generated_ids[:, ids.shape[-1]:][0], \n",
    "                #                                             skip_special_tokens=True)))\n",
    "\n",
    "                #for hm in generated_ids[:, ids.shape[-1]:]:\n",
    "                #    print(\"DialoGPT: {}\".format(tokenizer.decode(hm, skip_special_tokens=False)))\n",
    "            step = step + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50639c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DialoGPTTrainer(dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\" ):\n",
    "\n",
    "    \"\"\"\n",
    "    T5 trainer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    #tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "    \"\"\"\n",
    "    tokenizer.add_special_tokens({\n",
    "      \"eos_token\": \"</s>\",\n",
    "      \"bos_token\": \"<s>\",\n",
    "      \"unk_token\": \"<unk>\",\n",
    "      \"mask_token\": \"<mask>\",\n",
    "      \"pad_token\": \"<pad>\"\n",
    "    })\n",
    "    \"\"\"\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    #model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_params[\"MODEL\"])\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_params[\"MODEL\"], bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    # logging\n",
    "    console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "    # Importing the raw dataset\n",
    "    dataframe = dataframe[[source_text,target_text]]\n",
    "    display_df(dataframe.head(2))\n",
    "\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
    "    train_size = 0.8\n",
    "    train_dataset=dataframe.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
    "    val_dataset=dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = DataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "    val_set = DataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "      'shuffle': True,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "    val_params = {\n",
    "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "      'shuffle': False,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "    \n",
    "    \"\"\"\n",
    "    for eg in val_loader:\n",
    "        print(eg)\n",
    "        break\n",
    "    return 1\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    console.log(f'[Initiating Fine Tuning]...\\n')\n",
    "\n",
    "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "      train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "    console.log(f\"[Saving Model]...\\n\")\n",
    "    #Saving the model after training\n",
    "    path = os.path.join(output_dir, \"model_files\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "\n",
    "    # evaluating test dataset\n",
    "    console.log(f\"[Initiating Validation]...\\n\")\n",
    "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "        knowledge_sent, predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        final_df = pd.DataFrame({'Knowledge Sentence': knowledge_sent, 'Generated Text':predictions,'Actual Text':actuals})\n",
    "        final_df.to_csv(os.path.join(output_dir,'predictions.csv'))\n",
    "\n",
    "    console.save_text(os.path.join(output_dir,'logs.txt'))\n",
    "\n",
    "    console.log(f\"[Validation Completed.]\\n\")\n",
    "    console.print(f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\")\n",
    "    console.print(f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\")\n",
    "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")\n",
    "    \n",
    "    return {\"tokenizer\": tokenizer, \"model\": model}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8877fe",
   "metadata": {},
   "source": [
    "<h3>Load dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca3f0d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('wizards-of-wikipedia-data-extraction/out.json', 'r') as json_file:\n",
    "#    raw_dataset = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ff6a3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             prev_turn  \\\n",
      "0                                                        \n",
      "1           I'm a huge fan of science fiction myself!    \n",
      "2    I agree. One of my favorite forms of science f...   \n",
      "3    And that's difficult to do when dealing with t...   \n",
      "4    Thank you for the suggestion, I will definitel...   \n",
      "..                                                 ...   \n",
      "495  Do you have a preference as to the type of hot...   \n",
      "496  I think I prefer corndogs. It's less unwieldy ...   \n",
      "497  Ooh, that just happened today! Joey Chestnut a...   \n",
      "498  I've never been to a Nathan's but I bet that w...   \n",
      "499                                                      \n",
      "\n",
      "                                            human_sent  \\\n",
      "0    I think science fiction is an amazing genre fo...   \n",
      "1    Awesome! I really love how sci-fi storytellers...   \n",
      "2    It's not quite sci-fi, but my favorite version...   \n",
      "3    If you really want a look at the potential neg...   \n",
      "4    It blends science fiction and paranormal/psych...   \n",
      "..                                                 ...   \n",
      "495  I prefer corndogs! speccially when they were b...   \n",
      "496  I like them with just mustar like they served ...   \n",
      "497  Yes! he was born the same date as me, november...   \n",
      "498  It is unique in nathans because they serve hot...   \n",
      "499                       Hi, I love mountain climbing   \n",
      "\n",
      "                                        knowledge_sent  \n",
      "0    Science fiction (often shortened to SF or sci-...  \n",
      "1    Science fiction films have often been used to ...  \n",
      "2    The central premise for these stories oftentim...  \n",
      "3    Science fiction often explores the potential c...  \n",
      "4                                     no_passages_used  \n",
      "..                                                 ...  \n",
      "495  The 'korn dogs' were baked in a corn batter an...  \n",
      "496  The Nathan's Hot Dog Eating Contest is an annu...  \n",
      "497  Joseph \"Joey\" Christian Chestnut (born Novembe...  \n",
      "498  Traditionally, eating contests, often involvin...  \n",
      "499  The term mountaineering describes the sport of...  \n",
      "\n",
      "[500 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('../wizards-of-wikipedia-data-extraction/out.pkl')\n",
    "print(df[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85992105",
   "metadata": {},
   "source": [
    "<h2>Run the model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d917ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distilgpt2\n",
    "source_text_key = 'knowledge_sent'\n",
    "target_text_key = 'human_sent'\n",
    "output_dir = \"./outputs\"\n",
    "model_params={\n",
    "    \"MODEL\":\"../../models/DialoGPT-medium\",             # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\":1,          # training batch size\n",
    "    \"VALID_BATCH_SIZE\":1,          # validation batch size\n",
    "    \"TRAIN_EPOCHS\":1,              # number of training epochs\n",
    "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "    \"LEARNING_RATE\":2e-4,          # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\":64,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\":64,   # max length of target text\n",
    "    \"SEED\": 42                     # set seed for reproducibility \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "605f8d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[17:11:20] </span><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span>: Loading ..<span style=\"color: #800080; text-decoration-color: #800080\">/../models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">DialoGPT-medium...</span>                  <a href=\"file:///tmp/ipykernel_50675/2418050184.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2418050184.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_50675/2418050184.py#14\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14</span></a>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[17:11:20]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m: Loading ..\u001b[35m/../models/\u001b[0m\u001b[95mDialoGPT-medium...\u001b[0m                  \u001b]8;id=909384;file:///tmp/ipykernel_50675/2418050184.py\u001b\\\u001b[2m2418050184.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=654320;file:///tmp/ipykernel_50675/2418050184.py#14\u001b\\\u001b[2m14\u001b[0m\u001b]8;;\u001b\\\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[17:11:23] </span><span style=\"font-weight: bold\">[</span>Data<span style=\"font-weight: bold\">]</span>: Reading data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                           <a href=\"file:///tmp/ipykernel_50675/2418050184.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2418050184.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_50675/2418050184.py#39\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">39</span></a>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[17:11:23]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mData\u001b[1m]\u001b[0m: Reading data\u001b[33m...\u001b[0m                                           \u001b]8;id=189516;file:///tmp/ipykernel_50675/2418050184.py\u001b\\\u001b[2m2418050184.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=303309;file:///tmp/ipykernel_50675/2418050184.py#39\u001b\\\u001b[2m39\u001b[0m\u001b]8;;\u001b\\\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                         Sample Data                                         </span>\n",
       "+-------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">                source_text                  </span>|<span style=\"font-weight: bold\">                 target_text                 </span>|\n",
       "|---------------------------------------------+---------------------------------------------|\n",
       "| Science fiction (often shortened to SF or   | I think science fiction is an amazing genre |\n",
       "| sci-fi) is a genre of speculative fiction,  |  for anything. Future science, technology,  |\n",
       "|typically dealing with imaginative concepts  |  time travel, FTL travel, they're all such  |\n",
       "| such as futuristic science and technology,  |            interesting concepts.            |\n",
       "|space travel, time travel, faster than light |                                             |\n",
       "|      travel, parallel universes, and        |                                             |\n",
       "|           extraterrestrial life.            |                                             |\n",
       "+-------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                         Sample Data                                         \u001b[0m\n",
       "+-------------------------------------------------------------------------------------------+\n",
       "|\u001b[1m                source_text                 \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                target_text                 \u001b[0m|\n",
       "|---------------------------------------------+---------------------------------------------|\n",
       "| Science fiction (often shortened to SF or   | I think science fiction is an amazing genre |\n",
       "| sci-fi) is a genre of speculative fiction,  |  for anything. Future science, technology,  |\n",
       "|typically dealing with imaginative concepts  |  time travel, FTL travel, they're all such  |\n",
       "| such as futuristic science and technology,  |            interesting concepts.            |\n",
       "|space travel, time travel, faster than light |                                             |\n",
       "|      travel, parallel universes, and        |                                             |\n",
       "|           extraterrestrial life.            |                                             |\n",
       "+-------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">FULL Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "FULL Dataset: \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TRAIN Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TRAIN Dataset: \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TEST Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TEST Dataset: \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Initiating Fine Tuning<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                       <a href=\"file:///tmp/ipykernel_50675/2418050184.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2418050184.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_50675/2418050184.py#94\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">94</span></a>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Fine Tuning\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                       \u001b]8;id=335184;file:///tmp/ipykernel_50675/2418050184.py\u001b\\\u001b[2m2418050184.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=981409;file:///tmp/ipykernel_50675/2418050184.py#94\u001b\\\u001b[2m94\u001b[0m\u001b]8;;\u001b\\\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                      </span>\n",
       "+---------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                   Loss                   </span>|\n",
       "|------+-------+------------------------------------------|\n",
       "|  0   |   0   | tensor(7.7906, grad_fn=&lt;NllLossBackward&gt;)|\n",
       "+---------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                      \u001b[0m\n",
       "+---------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                  Loss                   \u001b[0m|\n",
       "|------+-------+------------------------------------------|\n",
       "|  0   |   0   | tensor(7.7906, grad_fn=<NllLossBackward>)|\n",
       "+---------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[17:11:26] </span><span style=\"font-weight: bold\">[</span>Saving Model<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                 <a href=\"file:///tmp/ipykernel_50675/2418050184.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2418050184.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_50675/2418050184.py#99\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">99</span></a>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[17:11:26]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mSaving Model\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                                 \u001b]8;id=838570;file:///tmp/ipykernel_50675/2418050184.py\u001b\\\u001b[2m2418050184.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=348015;file:///tmp/ipykernel_50675/2418050184.py#99\u001b\\\u001b[2m99\u001b[0m\u001b]8;;\u001b\\\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[17:11:28] </span><span style=\"font-weight: bold\">[</span>Initiating Validation<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                       <a href=\"file:///tmp/ipykernel_50675/2418050184.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2418050184.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_50675/2418050184.py#107\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">107</span></a>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[17:11:28]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Validation\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                       \u001b]8;id=187528;file:///tmp/ipykernel_50675/2418050184.py\u001b\\\u001b[2m2418050184.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=879882;file:///tmp/ipykernel_50675/2418050184.py#107\u001b\\\u001b[2m107\u001b[0m\u001b]8;;\u001b\\\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Validation Completed.<span style=\"font-weight: bold\">]</span>                                          <a href=\"file:///tmp/ipykernel_50675/2418050184.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2418050184.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_50675/2418050184.py#115\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115</span></a>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mValidation Completed.\u001b[1m]\u001b[0m                                          \u001b]8;id=912206;file:///tmp/ipykernel_50675/2418050184.py\u001b\\\u001b[2m2418050184.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=330658;file:///tmp/ipykernel_50675/2418050184.py#115\u001b\\\u001b[2m115\u001b[0m\u001b]8;;\u001b\\\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span> Model saved @ .<span style=\"color: #800080; text-decoration-color: #800080\">/outputs/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">model_files</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m Model saved @ .\u001b[35m/outputs/\u001b[0m\u001b[95mmodel_files\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Validation<span style=\"font-weight: bold\">]</span> Generation on Validation data saved @ .<span style=\"color: #800080; text-decoration-color: #800080\">/outputs/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">predictions.csv</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mValidation\u001b[1m]\u001b[0m Generation on Validation data saved @ .\u001b[35m/outputs/\u001b[0m\u001b[95mpredictions.csv\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Logs<span style=\"font-weight: bold\">]</span> Logs saved @ .<span style=\"color: #800080; text-decoration-color: #800080\">/outputs/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">logs.txt</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mLogs\u001b[1m]\u001b[0m Logs saved @ .\u001b[35m/outputs/\u001b[0m\u001b[95mlogs.txt\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TRAINING TIME ELAPSED: 8.450122989001102 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "model_out = DialoGPTTrainer(dataframe=df[:1], source_text=\"knowledge_sent\", target_text=\"human_sent\", model_params=model_params, output_dir=output_dir)\n",
    "end = timer()\n",
    "print(f\"\\n\\nTRAINING TIME ELAPSED: {end-start} seconds\")\n",
    "model = model_out['model']\n",
    "tokenizer = model_out['tokenizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "965461de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP #0\n",
      "\n",
      "Generated ids len: torch.Size([1, 64])\n",
      "source_ids len: torch.Size([1, 64])\n",
      "input_ids len: torch.Size([1, 64])\n",
      "INPUT IDS: tensor([[ 2061,   466,   345,   892,   286,   616, 25336,   983,    30,   220,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256]])\n",
      "\n",
      "Predictions: [\"I don't know, I'm not a memer.\"]\n",
      "Prompt: ['What do you think of my meme game? ']\n",
      "\n",
      "STEP #1\n",
      ">> user:What is the weather like?\n",
      "\n",
      "Generated ids len: torch.Size([1, 13])\n",
      "source_ids len: torch.Size([1, 64])\n",
      "input_ids len: torch.Size([1, 77])\n",
      "INPUT IDS: tensor([[   40,   836,   470,   760,   837,   314,  1101,   407,   257,  1066,\n",
      "           263,   764, 50256,  2061,   318,   262,  6193,   588,    30, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 77]' is invalid for input of size 128",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_50675/3003672247.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#test_src_text = \"Do you like cycling?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_target_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Of the three primary colors, Blue is my favorite\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'knowledge_sent'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'human_sent'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_src_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_50675/1153903423.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(tokenizer, model, source_text_key, target_text_key, source_text, target_text, model_params)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nGenerated ids len: {past_gen_ids.shape if step > 0 else source_ids.shape}\\nsource_ids len: {source_ids.shape}\\ninput_ids len: {input_ids.shape}\\nINPUT IDS: {input_ids}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 generated_ids = model.generate(\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1051\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_beams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_encoder_decoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m             )\n\u001b[0;32m-> 1053\u001b[0;31m             return self.beam_search(\n\u001b[0m\u001b[1;32m   1054\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1787\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   1788\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m    955\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mposition_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m             \u001b[0mposition_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpast_key_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 77]' is invalid for input of size 128"
     ]
    }
   ],
   "source": [
    "#test_src_text = \"Science fiction (often shortened to SF or sci-fi) is a genre of speculative fiction, typically dealing with imaginative concepts such as futuristic science and technology, space travel, time travel, faster than light travel, parallel universes, and extraterrestrial life\"\n",
    "#test_target_text = \"I think science fiction is an amazing genre for anything. Future science, technology, time travel, FTL trave;, they're all such interesting concepts\"\n",
    "test_src_text = \"What do you think of my meme game?\"\n",
    "#\"Situated on the bank of the Chenab River, Multan is Pakistan's 7th largest city and is the major cultural and economic centre of Southern Punjab. Multan's history stretches deep into antiquity. The ancient city was site of the renowned Hindu Multan Sun Temple, and was besieged by Alexander the Great during the Mallian Campaign.\"\n",
    "#test_src_text = \"Do you like cycling?\"\n",
    "test_target_text = \"Of the three primary colors, Blue is my favorite\"\n",
    "predict(tokenizer, model, 'knowledge_sent', 'human_sent', test_src_text + \" \" + tokenizer.eos_token, test_target_text, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ef6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(tokenizer, model, 'knowledge_sent', 'human_sent', in_msg + \" \" + tokenizer.eos_token, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef94186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9de71040",
   "metadata": {},
   "source": [
    "<h3>Basic DialoGPT run</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f9e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "t_tokenizer = AutoTokenizer.from_pretrained(\"../../models/DialoGPT-large\")\n",
    "t_model = AutoModelForCausalLM.from_pretrained(\"../../models/DialoGPT-large\")\n",
    "\n",
    "t_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's chat for 5 lines\n",
    "for step in range(10):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = t_tokenizer.encode(input(\">> User:\") + t_tokenizer.eos_token, return_tensors='pt')\n",
    "    hist_id = t_tokenizer.encode(\"Matt Damon and John Lennon starred in Fast and Furious\" + t_tokenizer.eos_token, return_tensors='pt')\n",
    "    \n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([hist_id, new_user_input_ids], dim=-1) if step > -1 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    print(f\"\\ninput ids len: {bot_input_ids.shape}\\nchat hist id len: {chat_history_ids.shape if step > 0 else new_user_input_ids.shape}\\nuser input id len: {new_user_input_ids.shape}\\n\")\n",
    "    chat_history_ids = t_model.generate(bot_input_ids, max_length=1000, pad_token_id=t_tokenizer.eos_token_id,\n",
    "                                       temperature=5, num_beams=4,\n",
    "                    repetition_penalty=2.5, num_return_sequences=3)\n",
    "    \n",
    "    prompt = [t_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in bot_input_ids]\n",
    "\n",
    "    #print(f\"\\nPROMPT: {prompt}\\n{bot_input_ids}\\n\\n{chat_history_ids}\")\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    for hm in chat_history_ids[:, bot_input_ids.shape[-1]:]:\n",
    "        print(\"DialoGPT: {}\".format(t_tokenizer.decode(hm, skip_special_tokens=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5709abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tokenizer = AutoTokenizer.from_pretrained(\"../models/DialoGPT-small/saves\")\n",
    "t_model = AutoModelForCausalLM.from_pretrained(\"../models/DialoGPT-small/saves\")\n",
    "help(t_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feab5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11808511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e87ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d0919e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae800eb",
   "metadata": {},
   "source": [
    "<h1>Fine-tuning someGPT on Dialog task</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e44f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farjad/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2021-12-01 22:21:16.753182: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-01 22:21:16.753223: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "cuda.empty_cache()\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "### For displaying\n",
    "\n",
    "import os\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "\n",
    "# define a rich console logger\n",
    "console=Console(record=True)\n",
    "\n",
    "def display_df(df):\n",
    "  \"\"\"display dataframe in ASCII format\"\"\"\n",
    "\n",
    "  console=Console()\n",
    "  table = Table(Column(\"source_text\", justify=\"center\" ), Column(\"target_text\", justify=\"center\"), title=\"Sample Data\",pad_edge=False, box=box.ASCII)\n",
    "\n",
    "  for i, row in enumerate(df.values.tolist()):\n",
    "    table.add_row(row[0], row[1])\n",
    "\n",
    "  console.print(table)\n",
    "\n",
    "training_logger = Table(Column(\"Epoch\", justify=\"center\" ), \n",
    "                        Column(\"Steps\", justify=\"center\"),\n",
    "                        Column(\"Loss\", justify=\"center\"), \n",
    "                        title=\"Training Status\",pad_edge=False, box=box.ASCII)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0609f15c",
   "metadata": {},
   "source": [
    "<h3>Load dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "252bda64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               prev_turn  \\\n",
      "0                                                          \n",
      "1             I'm a huge fan of science fiction myself!    \n",
      "2      I agree. One of my favorite forms of science f...   \n",
      "3      And that's difficult to do when dealing with t...   \n",
      "4      Thank you for the suggestion, I will definitel...   \n",
      "...                                                  ...   \n",
      "82717  I love heavy metal music. My favorite bands ar...   \n",
      "82718  Awesome, it helps me relax and relieve my stre...   \n",
      "82719  It's a great form of exercise as well. I usual...   \n",
      "82720  Yeah, that would be awesome. At least Ozzy Osb...   \n",
      "82721  Yeah metal wouldn't be the same without Ozzy. ...   \n",
      "\n",
      "                                              human_sent  \\\n",
      "0      I think science fiction is an amazing genre fo...   \n",
      "1      Awesome! I really love how sci-fi storytellers...   \n",
      "2      It's not quite sci-fi, but my favorite version...   \n",
      "3      If you really want a look at the potential neg...   \n",
      "4      It blends science fiction and paranormal/psych...   \n",
      "...                                                  ...   \n",
      "82717  That's cool! I like heavy metal too, especiall...   \n",
      "82718  Ah yes, moshing is a staple at punk and heavy ...   \n",
      "82719  Sounds fun! I kinda wish I was around in the l...   \n",
      "82720  Lol oh yes. In fact, his longevity and success...   \n",
      "82721  Totally! What's crazy is that he was fired fro...   \n",
      "\n",
      "                                          knowledge_sent  \n",
      "0      Science fiction (often shortened to SF or sci-...  \n",
      "1      Science fiction films have often been used to ...  \n",
      "2      The central premise for these stories oftentim...  \n",
      "3      Science fiction often explores the potential c...  \n",
      "4                                       no_passages_used  \n",
      "...                                                  ...  \n",
      "82717  Heavy metal (or simply metal) is a genre of ro...  \n",
      "82718  Since then, moshing has occasionally been perf...  \n",
      "82719  In 1968, three of the genre's most famous acts...  \n",
      "82720  His longevity and success have earned him the ...  \n",
      "82721  He was fired from Black Sabbath in 1979 and we...  \n",
      "\n",
      "[82722 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('wizards-of-wikipedia-data-extraction/out.pkl')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a0592f",
   "metadata": {},
   "source": [
    "<h3>Dataset Class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4536fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and \n",
    "    loading it into the dataloader to pass it to the neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "\n",
    "        #cleaning data so as to ensure data is in string type\n",
    "        source_text = ' '.join(source_text.split())\n",
    "        target_text = ' '.join(target_text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        temp = {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6db3a",
   "metadata": {},
   "source": [
    "<h2>Model Implementation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4ae4d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT(dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\" ):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "    tokenizer.add_special_tokens({\n",
    "      \"eos_token\": \"</s>\",\n",
    "      \"bos_token\": \"<s>\",\n",
    "      \"unk_token\": \"<unk>\",\n",
    "      \"pad_token\": \"<pad>\",\n",
    "      \"mask_token\": \"<mask>\"\n",
    "    })\n",
    "    # creating the configurations from which the model can be made\n",
    "    config = GPT2Config(\n",
    "      vocab_size=tokenizer.vocab_size,\n",
    "      bos_token_id=tokenizer.bos_token_id,\n",
    "      eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    # creating the model\n",
    "    model = TFGPT2LMHeadModel(config)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    dataframe = dataframe[[source_text,target_text]]\n",
    "    display_df(dataframe.head(2))\n",
    "\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
    "    train_size = 0.8\n",
    "    train_dataset=dataframe.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
    "    val_dataset=dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "    inputs = tokenizer.batch_encode_plus(train_dataset[source_text], max_length= model_params[\"MAX_SOURCE_TEXT_LENGTH\"], pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt').data\n",
    "    print(train_dataset[source_text][0])\n",
    "    print(len(inputs['input_ids']))\n",
    "    print(inputs)\n",
    "    labels = tokenizer.batch_encode_plus(train_dataset[target_text], max_length= model_params[\"MAX_TARGET_TEXT_LENGTH\"], pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt').data\n",
    "    print(train_dataset[target_text][0])\n",
    "    print(len(labels['input_ids']))\n",
    "    print(labels)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "    #print(dataset)\n",
    "    \n",
    "    #return\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    #training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "    #val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "\n",
    "    \"\"\"\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "      'shuffle': True,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "    val_params = {\n",
    "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "      'shuffle': False,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "    \n",
    "    print(training_set[0:2])\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = dataset.shuffle(1000).batch(model_params[\"TRAIN_BATCH_SIZE\"], drop_remainder=True)\n",
    "    \n",
    "    \n",
    "    # COMPILE MODEL\n",
    "    # defining our optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "    # definining our loss function\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    # defining our metric which we want to observe\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    # compiling the model\n",
    "    model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])\n",
    "    \n",
    "    \n",
    "    # MODEL TRAINING\n",
    "    history = model.fit(dataset, epochs=model_params[\"TRAIN_EPOCHS\"])\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc272d7",
   "metadata": {},
   "source": [
    "<h3>Generate responses</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "841f5bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer):\n",
    "    # encoding the input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "    # getting out output\n",
    "    beam_output = model.generate(\n",
    "      input_ids,\n",
    "      max_length = model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "      num_beams = 5,\n",
    "      temperature = 0.7,\n",
    "      no_repeat_ngram_size=2,\n",
    "      num_return_sequences=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b55ed",
   "metadata": {},
   "source": [
    "<h3>Run the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7eb37b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                         Sample Data                                         </span>\n",
       "+-------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">                source_text                  </span>|<span style=\"font-weight: bold\">                 target_text                 </span>|\n",
       "|---------------------------------------------+---------------------------------------------|\n",
       "| Science fiction (often shortened to SF or   | I think science fiction is an amazing genre |\n",
       "| sci-fi) is a genre of speculative fiction,  |  for anything. Future science, technology,  |\n",
       "|typically dealing with imaginative concepts  |  time travel, FTL travel, they're all such  |\n",
       "| such as futuristic science and technology,  |            interesting concepts.            |\n",
       "|space travel, time travel, faster than light |                                             |\n",
       "|      travel, parallel universes, and        |                                             |\n",
       "|           extraterrestrial life.            |                                             |\n",
       "| Science fiction films have often been used  |      Awesome! I really love how sci-fi      |\n",
       "|to focus on political or social issues, and  |            storytellers focus on            |\n",
       "|  to explore philosophical issues like the   |  political/social/philosophical issues that |\n",
       "|              human condition.               |  would still be around even in the future.  |\n",
       "|                                             |            Makes them relatable.            |\n",
       "+-------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                         Sample Data                                         \u001b[0m\n",
       "+-------------------------------------------------------------------------------------------+\n",
       "|\u001b[1m                source_text                 \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                target_text                 \u001b[0m|\n",
       "|---------------------------------------------+---------------------------------------------|\n",
       "| Science fiction (often shortened to SF or   | I think science fiction is an amazing genre |\n",
       "| sci-fi) is a genre of speculative fiction,  |  for anything. Future science, technology,  |\n",
       "|typically dealing with imaginative concepts  |  time travel, FTL travel, they're all such  |\n",
       "| such as futuristic science and technology,  |            interesting concepts.            |\n",
       "|space travel, time travel, faster than light |                                             |\n",
       "|      travel, parallel universes, and        |                                             |\n",
       "|           extraterrestrial life.            |                                             |\n",
       "| Science fiction films have often been used  |      Awesome! I really love how sci-fi      |\n",
       "|to focus on political or social issues, and  |            storytellers focus on            |\n",
       "|  to explore philosophical issues like the   |  political/social/philosophical issues that |\n",
       "|              human condition.               |  would still be around even in the future.  |\n",
       "|                                             |            Makes them relatable.            |\n",
       "+-------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">FULL Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "FULL Dataset: \u001b[1m(\u001b[0m\u001b[1;36m2048\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TRAIN Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1638</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TRAIN Dataset: \u001b[1m(\u001b[0m\u001b[1;36m1638\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TEST Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">410</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TEST Dataset: \u001b[1m(\u001b[0m\u001b[1;36m410\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some, including ice cream trucks, sell frozen or prepackaged food; others have on-board kitchens and prepare food from scratch.\n",
      "1638\n",
      "{'input_ids': tensor([[ 4366,    11,  1390,  ..., 50259, 50259, 50259],\n",
      "        [ 3919,    62,  6603,  ..., 50259, 50259, 50259],\n",
      "        [32697,  5278, 26698,  ..., 50259, 50259, 50259],\n",
      "        ...,\n",
      "        [ 2025, 18284,  1097,  ..., 50259, 50259, 50259],\n",
      "        [14573,   318,   530,  ..., 50259, 50259, 50259],\n",
      "        [ 1026,   318,  2408,  ..., 50259, 50259, 50259]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "I understand your skepticism. However most have on board kitchens so its just like a normal restaurant on wheels.\n",
      "1638\n",
      "{'input_ids': tensor([[   40,  1833,   534,  ..., 50259, 50259, 50259],\n",
      "        [   69,  3532,  6473,  ..., 50259, 50259, 50259],\n",
      "        [ 5297,     0, 29454,  ..., 50259, 50259, 50259],\n",
      "        ...,\n",
      "        [ 8567,    11,   257,  ..., 50259, 50259, 50259],\n",
      "        [14573,   318,   616,  ..., 50259, 50259, 50259],\n",
      "        [18357,   257,  6853,  ..., 50259, 50259, 50259]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/farjad/.local/lib/python3.8/site-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /home/farjad/.local/lib/python3.8/site-packages/keras/engine/training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/farjad/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/farjad/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/farjad/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/farjad/.local/lib/python3.8/site-packages/keras/engine/training.py:835 run_step  **\n        outputs = model.train_step(data)\n    /home/farjad/.local/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:772 train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    /home/farjad/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py:436 update_state\n        self.build(y_pred, y_true)\n    /home/farjad/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py:358 build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(y_pred, self._get_metric_objects,\n    /home/farjad/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1376 map_structure_up_to\n        return map_structure_with_tuple_paths_up_to(\n    /home/farjad/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1458 map_structure_with_tuple_paths_up_to\n        assert_shallow_structure(\n    /home/farjad/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1091 assert_shallow_structure\n        raise ValueError(\n\n    ValueError: The two structures don't have the same sequence length. Input structure has length 4, while shallow structure has length 2.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40956/3088722415.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"SEED\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m42\u001b[0m                     \u001b[0;31m# set seed for reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m }\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'myGPT_out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_40956/2498669380.py\u001b[0m in \u001b[0;36mGPT\u001b[0;34m(dataframe, source_text, target_text, model_params, output_dir)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# MODEL TRAINING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TRAIN_EPOCHS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 759\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    760\u001b[0m             *args, **kwds))\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/farjad/.local/lib/python3.8/site-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /home/farjad/.local/lib/python3.8/site-packages/keras/engine/training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/farjad/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/farjad/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/farjad/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/farjad/.local/lib/python3.8/site-packages/keras/engine/training.py:835 run_step  **\n        outputs = model.train_step(data)\n    /home/farjad/.local/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:772 train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    /home/farjad/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py:436 update_state\n        self.build(y_pred, y_true)\n    /home/farjad/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py:358 build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(y_pred, self._get_metric_objects,\n    /home/farjad/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1376 map_structure_up_to\n        return map_structure_with_tuple_paths_up_to(\n    /home/farjad/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1458 map_structure_with_tuple_paths_up_to\n        assert_shallow_structure(\n    /home/farjad/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py:1091 assert_shallow_structure\n        raise ValueError(\n\n    ValueError: The two structures don't have the same sequence length. Input structure has length 4, while shallow structure has length 2.\n"
     ]
    }
   ],
   "source": [
    "src_text = 'knowledge_sent'\n",
    "out_text = 'human_sent'\n",
    "model_params={\n",
    "    \"MODEL\":\"../models/DialoGPT-small/saves\",             # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\":12,          # training batch size\n",
    "    \"VALID_BATCH_SIZE\":1,          # validation batch size\n",
    "    \"TRAIN_EPOCHS\":1,              # number of training epochs\n",
    "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "    \"LEARNING_RATE\":2e-4,          # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\":512,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\":512,   # max length of target text\n",
    "    \"SEED\": 42                     # set seed for reproducibility \n",
    "}\n",
    "model, tokenizer = GPT(df[:2048], src_text, out_text, model_params, 'myGPT_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629b82f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3368f967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello everyone! </s> How are you doing? </s> I'm doing well, what about you? </s> All good, thank you </s>\n",
      "[[15496, 2506, 0, 50257, 2437, 389, 345, 1804, 30, 50257], [40, 1101, 1804, 880, 11, 644, 546, 345, 30, 50257]]\n",
      "\n",
      "Hello everyone! </s> How are you doing?\n",
      "[15496, 2506, 0, 50257, 2437, 389, 345, 1804, 30]\n",
      "\n",
      "[[2506, 0, 50257, 2437, 389, 345, 1804, 30, 50257], [1101, 1804, 880, 11, 644, 546, 345, 30, 50257]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "n: 15496, dec: Hello\n",
      "n: 2506, dec:  everyone\n",
      "n: 0, dec: !\n",
      "n: 50257, dec: </s>\n",
      "n: 2437, dec: How\n",
      "n: 389, dec:  are\n",
      "n: 345, dec:  you\n",
      "n: 1804, dec:  doing\n",
      "n: 30, dec: ?\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"../models/DialoGPT-small/saves\")\n",
    "tokenizer.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})\n",
    "single_string = ''\n",
    "single_string += \"Hello everyone!\" + tokenizer.eos_token\n",
    "single_string += \"How are you doing?\" + tokenizer.eos_token\n",
    "single_string += \"I'm doing well, what about you?\" + tokenizer.eos_token\n",
    "single_string += \"All good, thank you\" + tokenizer.eos_token\n",
    "string_tokenized = tokenizer.encode(single_string)\n",
    "print(tokenizer.decode(string_tokenized))\n",
    "\n",
    "examples = []\n",
    "block_size = 10\n",
    "BATCH_SIZE = 12\n",
    "BUFFER_SIZE = 1000\n",
    "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
    "  examples.append(string_tokenized[i:i + block_size])\n",
    "\n",
    "print(examples)\n",
    "inputs, labels = [], []\n",
    "for ex in examples:\n",
    "  inputs.append(ex[:-1])\n",
    "  labels.append(ex[1:])\n",
    "\n",
    "print(f\"\\n{tokenizer.decode(inputs[0])}\\n{inputs[0]}\\n\")\n",
    "print(labels)\n",
    "print(\"\\n\\n\\n\")\n",
    "for num in inputs[0]:\n",
    "    print(f\"n: {num}, dec: {tokenizer.decode(num)}\")\n",
    "print(\"\\n\\n\\n\")\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412c625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

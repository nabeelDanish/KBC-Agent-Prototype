{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e507da9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2021-12-05 16:14:25.421158: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-05 16:14:25.421198: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import cuda\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "\n",
    "\n",
    "training_logger = Table(Column(\"Epoch\", justify=\"center\"),\n",
    "                        Column(\"Steps\", justify=\"center\"),\n",
    "                        Column(\"Loss\", justify=\"center\"),\n",
    "                        title=\"Training Status\", pad_edge=False, box=box.ASCII)\n",
    "console = Console(record=True)\n",
    "\n",
    "class PTDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and\n",
    "    loading it into the dataloader to pass it to the neural network for fine-tuning the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "\n",
    "        # cleaning data so as to ensure data is in string type\n",
    "        source_text = ' '.join(source_text.split())\n",
    "        target_text = ' '.join(target_text.split())\n",
    "\n",
    "        # source = self.tokenizer.batch_encode_plus([source_text], return_tensors='pt')\n",
    "        # target = self.tokenizer.batch_encode_plus([target_text], return_tensors='pt')\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([source_text], max_length=self.source_len, pad_to_max_length=True,\n",
    "                                                  truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([target_text], max_length=self.summ_len, pad_to_max_length=True,\n",
    "                                                  truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        temp = {\n",
    "            'source_ids': source_ids.to(dtype=torch.long),\n",
    "            'source_mask': source_mask.to(dtype=torch.long),\n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }\n",
    "\n",
    "        return temp\n",
    "\n",
    "\n",
    "def display_df(df):\n",
    "    \"\"\"display dataframe in ASCII format\"\"\"\n",
    "    df_console = Console()\n",
    "    table = Table(Column(\"source_text\", justify=\"center\"), Column(\"target_text\", justify=\"center\"),\n",
    "                  title=\"Sample Data\",\n",
    "                  pad_edge=False, box=box.ASCII)\n",
    "\n",
    "    for i, row in enumerate(df.values.tolist()):\n",
    "        table.add_row(row[0], row[1])\n",
    "\n",
    "    df_console.print(table)\n",
    "\n",
    "\n",
    "class DialoGPTController:\n",
    "    def __init__(self, model_path):\n",
    "        # define a rich console logger\n",
    "        self.TAG = 'DialoGPTController'\n",
    "\n",
    "        # Not enough memory to train this model, which is why the cuda option is commented out\n",
    "        # self.device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "        # cuda.empty_cache()\n",
    "        self.device = 'cpu'\n",
    "\n",
    "        # Conversation hasn't started, so set turn and chat history to None\n",
    "        self.turn = None\n",
    "        self.chat_history_ids = None\n",
    "\n",
    "        self.model_path = model_path\n",
    "\n",
    "        # Initial values for model and tokenizer\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def __initialize_model(self, refresh=False):\n",
    "        # Loads model from disk into memory\n",
    "        console.log(f\"\"\"[{self.TAG}]: Loading {self.model_path}...\\n\"\"\")\n",
    "\n",
    "        if self.tokenizer is None or refresh:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "            self.tokenizer.add_special_tokens({'pad_token': self.tokenizer.eos_token})\n",
    "        if self.model is None or refresh:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(self.model_path, bos_token_id=self.tokenizer.bos_token_id,\n",
    "                                                              eos_token_id=self.tokenizer.eos_token_id)\n",
    "            self.model = self.model.to(self.device)\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "        print(f\"{self.TAG}: CUDA IS {'NOT' if not cuda.is_available() else ''} AVAILABLE\")\n",
    "\n",
    "    def predict(self, user_input, output_fragment=\"\", new_dialog_session=False):\n",
    "        # If this is the first turn, initialize the turn variable\n",
    "        if new_dialog_session or self.turn is None:\n",
    "            self.turn = 0\n",
    "\n",
    "        # Ensure the model is initialized\n",
    "        self.__initialize_model()\n",
    "\n",
    "        # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "        new_user_input_ids = self.tokenizer.encode(\n",
    "            user_input + self.tokenizer.eos_token + output_fragment, return_tensors='pt')\n",
    "\n",
    "        # append the new user input tokens to the chat history\n",
    "        bot_input_ids = torch.cat([self.chat_history_ids,\n",
    "                                   new_user_input_ids], dim=-1) if self.turn > 0 else new_user_input_ids\n",
    "\n",
    "        # generated a response while limiting the total chat history to 1000 tokens,\n",
    "        # print(\n",
    "        #    f\"\\ninput ids len: {bot_input_ids.shape}\\nchat hist id len: \"\n",
    "        #    f\"{self.chat_history_ids.shape if self.turn > 0 else new_user_input_ids.shape}\"\n",
    "        #    f\"\\nuser input id len: {new_user_input_ids.shape}\\n\")\n",
    "        chat_history_ids = self.model.generate(bot_input_ids, max_length=1000, pad_token_id=self.tokenizer.eos_token_id,\n",
    "                                               temperature=5, num_beams=4, repetition_penalty=2.5,\n",
    "                                               num_return_sequences=3)\n",
    "\n",
    "        # prompt = [self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        #          for g in bot_input_ids]\n",
    "        # print(f\"\\nPROMPT: {prompt}\\n{bot_input_ids}\\n\\n{chat_history_ids}\")\n",
    "\n",
    "        # print the decoded, generated output for this conversation turn\n",
    "        # model may have returned multiple responses\n",
    "        best_response = None\n",
    "        for response_id, response in enumerate(chat_history_ids[:, bot_input_ids.shape[-1]:]):\n",
    "            decoded_response = self.tokenizer.decode(response, skip_special_tokens=True)\n",
    "            if response_id == 0:\n",
    "                best_response = decoded_response\n",
    "            print(\"DialoGPT: {}\".format(decoded_response))\n",
    "\n",
    "        return best_response\n",
    "\n",
    "    def __train_step(self, loader, optimizer, epoch):\n",
    "        \"\"\"\n",
    "        Function to be called for training with the parameters passed from main function\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.train()\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(self.device, dtype=torch.long)\n",
    "            y_ids = y[:, :-1].contiguous()\n",
    "            lm_labels = y[:, 1:].clone().detach()\n",
    "            lm_labels[y[:, 1:] == self.tokenizer.pad_token_id] = -100\n",
    "            ids = data['source_ids'].to(self.device, dtype=torch.long)\n",
    "            mask = data['source_mask'].to(self.device, dtype=torch.long)\n",
    "\n",
    "            outputs = self.model(input_ids=ids, attention_mask=mask, labels=y)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            if _ % 10 == 0:\n",
    "                training_logger.add_row(str(epoch), str(_), str(loss))\n",
    "                console.print(training_logger)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def train(self, train_batch_size, valid_batch_size, train_epochs, val_epochs, learning_rate,\n",
    "              max_source_text_length, max_target_text_length, dataframe, source_text_key, target_text_key, output_dir):\n",
    "        \"\"\"\n",
    "        T5 trainer\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Set random seeds and deterministic pytorch for reproducibility\n",
    "        torch.manual_seed(42)  # pytorch random seed\n",
    "        np.random.seed(42)  # numpy random seed\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        # Importing the raw dataset\n",
    "        console.log(f\"[{self.TAG}]: Reading data...\\n\")\n",
    "        dataframe = dataframe[[source_text_key, target_text_key]]\n",
    "        display_df(dataframe.head(2))\n",
    "\n",
    "        # Creation of Dataset and Dataloader\n",
    "        # Defining the train size. So 80% of the data will be used for training and the rest for validation.\n",
    "        train_size = 0.8\n",
    "        train_dataset = dataframe.sample(frac=train_size, random_state=42)\n",
    "        val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "        train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "        console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "        console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "        console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "        # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "        training_set = PTDataSetClass(train_dataset, self.tokenizer, max_source_text_length,\n",
    "                                      max_target_text_length, source_text_key, target_text_key)\n",
    "        val_set = PTDataSetClass(val_dataset, self.tokenizer, self.max_source_text_length,\n",
    "                                 self.max_target_text_length, source_text_key, target_text_key)\n",
    "\n",
    "        # Defining the parameters for creation of data loaders\n",
    "        train_params = {\n",
    "            'batch_size': train_batch_size,\n",
    "            'shuffle': True,\n",
    "            'num_workers': 0\n",
    "        }\n",
    "\n",
    "        val_params = {\n",
    "            'batch_size': valid_batch_size,\n",
    "            'shuffle': False,\n",
    "            'num_workers': 0\n",
    "        }\n",
    "\n",
    "        # Creation of Dataloaders for testing and validation. This will be used down for training and validation\n",
    "        # stage for the model.\n",
    "        training_loader = DataLoader(training_set, **train_params)\n",
    "        val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "        # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
    "        optimizer = torch.optim.Adam(params=self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Training loop\n",
    "        console.log(f'[Initiating Fine Tuning]...\\n')\n",
    "\n",
    "        for epoch in range(train_epochs):\n",
    "            self.__train_step(training_loader, optimizer, epoch)\n",
    "\n",
    "        console.log(f\"[Saving Model]...\\n\")\n",
    "        # Saving the model after training\n",
    "        path = os.path.join(output_dir, \"model_files\")\n",
    "        self.model.save_pretrained(path)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "\n",
    "        # evaluating test dataset\n",
    "        console.log(f\"[Initiating Validation]...\\n\")\n",
    "        for epoch in range(val_epochs):\n",
    "            knowledge_sent, predictions, actuals = self.validate(epoch, val_loader)\n",
    "            final_df = pd.DataFrame(\n",
    "                {'Knowledge Sentence': knowledge_sent, 'Generated Text': predictions, 'Actual Text': actuals})\n",
    "            final_df.to_csv(os.path.join(output_dir, 'predictions.csv'))\n",
    "\n",
    "        console.save_text(os.path.join(output_dir, 'logs.txt'))\n",
    "\n",
    "        console.log(f\"[Validation Completed.]\\n\")\n",
    "        console.print(f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\")\n",
    "        console.print(\n",
    "            f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir, 'predictions.csv')}\\n\"\"\")\n",
    "        console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir, 'logs.txt')}\\n\"\"\")\n",
    "\n",
    "    def validate(self, epoch, loader):\n",
    "        \"\"\"\n",
    "        Function to evaluate model for predictions\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        prompts = []\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(loader, 0):\n",
    "                y = data['target_ids'].to(self.device, dtype=torch.long)\n",
    "                ids = data['source_ids'].to(self.device, dtype=torch.long)\n",
    "                mask = data['source_mask'].to(self.device, dtype=torch.long)\n",
    "\n",
    "                generated_ids = self.model.generate(\n",
    "                    input_ids=ids,\n",
    "                    attention_mask=mask,\n",
    "                    max_length=64,\n",
    "                    num_beams=2,\n",
    "                    repetition_penalty=2.5,\n",
    "                    length_penalty=1.0,\n",
    "                    early_stopping=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "                prompt = [self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in\n",
    "                          ids]\n",
    "                preds = [self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in\n",
    "                         generated_ids]\n",
    "                target = [self.tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in\n",
    "                          y]\n",
    "                if _ % 10 == 0:\n",
    "                    console.print(f'Completed {_}')\n",
    "\n",
    "                prompts.extend(prompt)\n",
    "                predictions.extend(preds)\n",
    "                actuals.extend(target)\n",
    "\n",
    "                # print(f\"Predictions: {predictions}\\nActuals: {actuals}\\nPrompt: {prompt}\")\n",
    "        return prompts, predictions, actuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b94ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DialoGPTController('../../models/DialoGPT-medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10d3649",
   "metadata": {},
   "source": [
    "<h3>Model training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30398d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5671442e",
   "metadata": {},
   "source": [
    "<h3>Inference: Dialog generation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3571bf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:South Africa\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[16:15:51] </span><span style=\"font-weight: bold\">[</span>DialoGPTController<span style=\"font-weight: bold\">]</span>: Loading ..<span style=\"color: #800080; text-decoration-color: #800080\">/../models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">DialoGPT-medium...</span>    <a href=\"file:///tmp/ipykernel_49799/3937628176.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3937628176.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_49799/3937628176.py#105\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">105</span></a>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[16:15:51]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mDialoGPTController\u001b[1m]\u001b[0m: Loading ..\u001b[35m/../models/\u001b[0m\u001b[95mDialoGPT-medium...\u001b[0m    \u001b]8;id=476027;file:///tmp/ipykernel_49799/3937628176.py\u001b\\\u001b[2m3937628176.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=589470;file:///tmp/ipykernel_49799/3937628176.py#105\u001b\\\u001b[2m105\u001b[0m\u001b]8;;\u001b\\\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPTController: CUDA IS  AVAILABLE\n",
      "DialoGPT: I'm from South Africa and I can confirm this.\n",
      "DialoGPT: I'm in South Africa too!\n",
      "DialoGPT: I'm from South Africa and I've never heard of it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm from South Africa and I can confirm this.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(input(\">> User:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae21281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[16:16:02] </span><span style=\"font-weight: bold\">[</span>DialoGPTController<span style=\"font-weight: bold\">]</span>: Loading ..<span style=\"color: #800080; text-decoration-color: #800080\">/../models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">DialoGPT-medium...</span>    <a href=\"file:///tmp/ipykernel_49799/3937628176.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3937628176.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_49799/3937628176.py#105\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">105</span></a>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[16:16:02]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mDialoGPTController\u001b[1m]\u001b[0m: Loading ..\u001b[35m/../models/\u001b[0m\u001b[95mDialoGPT-medium...\u001b[0m    \u001b]8;id=233237;file:///tmp/ipykernel_49799/3937628176.py\u001b\\\u001b[2m3937628176.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=868537;file:///tmp/ipykernel_49799/3937628176.py#105\u001b\\\u001b[2m105\u001b[0m\u001b]8;;\u001b\\\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPTController: CUDA IS  AVAILABLE\n",
      "DialoGPT: I think it was a guy who looked like him, but I could be wrong.\n",
      "DialoGPT: I think it was a guy who looked like him, but I'm not sure.\n",
      "DialoGPT: I think it was a guy who looked like him, but I don't remember his name.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I think it was a guy who looked like him, but I could be wrong.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"Who was the host?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d582a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User: What do you think about Cinematography?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[20:31:42] </span><span style=\"font-weight: bold\">[</span>DialoGPTController<span style=\"font-weight: bold\">]</span>: Loading ..<span style=\"color: #800080; text-decoration-color: #800080\">/../models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">DialoGPT-medium...</span>    <a href=\"file:///tmp/ipykernel_69421/3937628176.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3937628176.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_69421/3937628176.py#105\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">105</span></a>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[20:31:42]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mDialoGPTController\u001b[1m]\u001b[0m: Loading ..\u001b[35m/../models/\u001b[0m\u001b[95mDialoGPT-medium...\u001b[0m    \u001b]8;id=35156;file:///tmp/ipykernel_69421/3937628176.py\u001b\\\u001b[2m3937628176.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=997829;file:///tmp/ipykernel_69421/3937628176.py#105\u001b\\\u001b[2m105\u001b[0m\u001b]8;;\u001b\\\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPTController: CUDA IS  AVAILABLE\n",
      "DialoGPT: ive only seen a few movies, but i like it.\n",
      "DialoGPT: ive only seen a few movies, but i like the cinematography.\n",
      "DialoGPT: ive only seen a few of the movies, but i love it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ive only seen a few movies, but i like it.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(input(\">> User: \"), output_fragment=\"Cinematography is an art, \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db96c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

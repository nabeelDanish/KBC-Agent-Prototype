{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c277e9",
   "metadata": {},
   "source": [
    "<h1>Fine-tuning T5 on Sentence Generation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a130ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jt -t monokai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f37efd",
   "metadata": {},
   "source": [
    "<h3>Import Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eee5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelWithLMHead, DistilBertTokenizer, DistilBertForMaskedLM, AutoModelForSeq2SeqLM\n",
    "\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "\n",
    "# define a rich console logger\n",
    "console=Console(record=True)\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "cuda.empty_cache()\n",
    "\n",
    "import json\n",
    "\n",
    "def display_df(df):\n",
    "  \"\"\"display dataframe in ASCII format\"\"\"\n",
    "\n",
    "  console=Console()\n",
    "  table = Table(Column(\"source_text\", justify=\"center\" ), Column(\"target_text\", justify=\"center\"), title=\"Sample Data\",pad_edge=False, box=box.ASCII)\n",
    "\n",
    "  for i, row in enumerate(df.values.tolist()):\n",
    "    table.add_row(row[0], row[1])\n",
    "\n",
    "  console.print(table)\n",
    "\n",
    "training_logger = Table(Column(\"Epoch\", justify=\"center\" ), \n",
    "                        Column(\"Steps\", justify=\"center\"),\n",
    "                        Column(\"Loss\", justify=\"center\"), \n",
    "                        title=\"Training Status\",pad_edge=False, box=box.ASCII)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc6bf5",
   "metadata": {},
   "source": [
    "<h3>DataSetClass</h3>\n",
    "<h4>Custom dataset class for loading the dataset and passing it to the model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and \n",
    "    loading it into the dataloader to pass it to the neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "\n",
    "        #cleaning data so as to ensure data is in string type\n",
    "        source_text = ' '.join(source_text.split())\n",
    "        target_text = ' '.join(target_text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        temp = {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6c321",
   "metadata": {},
   "source": [
    "<h3>Train method</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to be called for training with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if _%10==0:\n",
    "            training_logger.add_row(str(epoch), str(_), str(loss))\n",
    "            console.print(training_logger)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4600631e",
   "metadata": {},
   "source": [
    "<h3>Validate method</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6fca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to evaluate model for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "      for _, data in enumerate(loader, 0):\n",
    "          y = data['target_ids'].to(device, dtype = torch.long)\n",
    "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "          generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=150, \n",
    "              num_beams=2,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True\n",
    "              )\n",
    "        \n",
    "          prompt = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in ids]\n",
    "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "          if _%10==0:\n",
    "              console.print(f'Completed {_}')\n",
    "\n",
    "          predictions.extend(preds)\n",
    "          actuals.extend(target)\n",
    "            \n",
    "          #print(f\"Predictions: {predictions}\\nActuals: {actuals}\\nPrompt: {prompt}\")\n",
    "    return prompt, predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tokenizer, model, source_text_key, target_text_key, source_text, target_text, model_params):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    val_params = {\n",
    "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "      'shuffle': False,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "    \n",
    "    data_src = pd.DataFrame([{'knowledge_sent': source_text, 'human_sent': target_text}])\n",
    "    data_loader = DataLoader(YourDataSetClass(data_src, tokenizer, 64, 64, source_text_key, target_text_key), **val_params)\n",
    "    #print(f\"data type: {data.testthis('target')}\")\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(data_loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                max_length=150, \n",
    "                num_beams=2\n",
    "              )\n",
    "            \n",
    "            print(f\"Generated ids: {generated_ids}\\n\\n\")\n",
    "\n",
    "            prompt = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in ids]\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "\n",
    "            print(f\"Predictions: {predictions[0]}\\nActuals: {actuals[0]}\\nPrompt: {prompt[0]}\")\n",
    "        return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50639c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def T5Trainer(dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\" ):\n",
    "\n",
    "    \"\"\"\n",
    "    T5 trainer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "    #tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_params[\"MODEL\"])\n",
    "    #model = AutoModelWithLMHead.from_pretrained(model_params[\"MODEL\"])\n",
    "    model = model.to(device)\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "    # Importing the raw dataset\n",
    "    dataframe = dataframe[[source_text,target_text]]\n",
    "    display_df(dataframe.head(2))\n",
    "\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
    "    train_size = 0.8\n",
    "    train_dataset=dataframe.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
    "    val_dataset=dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "    val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "      'shuffle': True,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "    val_params = {\n",
    "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "      'shuffle': False,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "    \n",
    "    \"\"\"\n",
    "    for eg in val_loader:\n",
    "        print(eg)\n",
    "        break\n",
    "    return 1\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    console.log(f'[Initiating Fine Tuning]...\\n')\n",
    "\n",
    "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "      train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "    console.log(f\"[Saving Model]...\\n\")\n",
    "    #Saving the model after training\n",
    "    path = os.path.join(output_dir, \"model_files\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "\n",
    "    # evaluating test dataset\n",
    "    console.log(f\"[Initiating Validation]...\\n\")\n",
    "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "        knowledge_sent, predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        final_df = pd.DataFrame({'Knowledge Sentence': knowledge_sent, 'Generated Text':predictions,'Actual Text':actuals})\n",
    "        final_df.to_csv(os.path.join(output_dir,'predictions.csv'))\n",
    "\n",
    "    console.save_text(os.path.join(output_dir,'logs.txt'))\n",
    "\n",
    "    console.log(f\"[Validation Completed.]\\n\")\n",
    "    console.print(f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\")\n",
    "    console.print(f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\")\n",
    "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")\n",
    "    \n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8877fe",
   "metadata": {},
   "source": [
    "<h3>Load dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f0d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wizards-of-wikipedia-data-extraction/out.json', 'r') as json_file:\n",
    "    raw_dataset = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('wizards-of-wikipedia-data-extraction/out.pkl')\n",
    "print(df[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85992105",
   "metadata": {},
   "source": [
    "<h2>Run the model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d917ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distilgpt2\n",
    "model_path = '../models/t5-small'\n",
    "source_text_key = 'knowledge_sent'\n",
    "target_text_key = 'human_sent'\n",
    "model_params={\n",
    "    \"MODEL\":model_path,             # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\":16,          # training batch size\n",
    "    \"VALID_BATCH_SIZE\":2,          # validation batch size\n",
    "    \"TRAIN_EPOCHS\":4,              # number of training epochs\n",
    "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "    \"LEARNING_RATE\":2e-4,          # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\":64,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\":64,   # max length of target text\n",
    "    \"SEED\": 42                     # set seed for reproducibility \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15649505",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = T5Trainer(dataframe=df[:1000], source_text=\"knowledge_sent\", target_text=\"human_sent\", model_params=model_params, output_dir=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf992011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_src_text = \"Science fiction (often shortened to SF or sci-fi) is a genre of speculative fiction, typically dealing with imaginative concepts such as futuristic science and technology, space travel, time travel, faster than light travel, parallel universes, and extraterrestrial life\"\n",
    "#test_target_text = \"I think science fiction is an amazing genre for anything. Future science, technology, time travel, FTL trave;, they're all such interesting concepts\"\n",
    "#test_src_text = \"Multan is a city and capital of Multan Division located in Punjab, Pakistan. Situated on the bank of the Chenab River, Multan is Pakistan's 7th largest city and is the major cultural and economic centre of Southern Punjab. Multan's history stretches deep into antiquity. The ancient city was site of the renowned Hindu Multan Sun Temple, and was besieged by Alexander the Great during the Mallian Campaign.\"\n",
    "path = './outputs/model_files'\n",
    "tokenizer = T5Tokenizer.from_pretrained(path, local_files_only=True)\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "    #tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = T5ForConditionalGeneration.from_pretrained(path, local_files_only=True)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_params[\"MODEL\"])\n",
    "#model = AutoModelWithLMHead.from_pretrained(model_params[\"MODEL\"])\n",
    "model = model.to(device)\n",
    "test_src_text = \"Do you like cycling?\"\n",
    "test_target_text = \"Of the three primary colors, Blue is my favorite\"\n",
    "predict(tokenizer, model, 'knowledge_sent', 'human_sent', test_src_text, test_target_text, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5577175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edacb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e243a69c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21528fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "input_ids = tokenizer(\"summarize: Six tournaments have so far been played, and only the West Indies, who currently hold the title, has won the tournament on multiple occasions. The inaugural 2007 World Twenty20, was staged in South Africa, and won by India, who defeated Pakistan in the final at the Wanderers Stadium in Johannesburg. The 2009 tournament took place in England, and was won by the previous runner-up, Pakistan, who defeated Sri Lanka in the final at Lord's. The third tournament was held in 2010, hosted by the countries making up the West Indies cricket team.\", return_tensors='pt', add_special_tokens=True).input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "for output in outputs:\n",
    "    print(tokenizer.decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16798198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelWithLMHead.from_pretrained(model_path)\n",
    "\n",
    "def paraphrase(text, max_length=128):\n",
    "\n",
    "  input_ids = tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "  generated_ids = model.generate(input_ids=input_ids, num_return_sequences=5, num_beams=5, max_length=max_length, no_repeat_ngram_size=2, repetition_penalty=3.5, length_penalty=1.0, early_stopping=True)\n",
    "\n",
    "  preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "\n",
    "  return preds\n",
    "  \n",
    "preds = paraphrase(\"paraphrase: What is the best framework for dealing with a huge text dataset?\")\n",
    "\n",
    "for pred in preds:\n",
    "  print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378ddf3e",
   "metadata": {},
   "source": [
    "<h3>T5 on fill-in-the-blank task</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3604a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "\n",
    "T5_PATH = '../models/t5-base' # \"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # My envirnment uses CPU\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(T5_PATH)\n",
    "t5_config = T5Config.from_pretrained(T5_PATH)\n",
    "t5_mlm = T5ForConditionalGeneration.from_pretrained(T5_PATH, config=t5_config).to(DEVICE)\n",
    "\n",
    "# Input text\n",
    "#text = \"Why was the tournament postponed? <extra_id_0> due to COVID-19. </s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677f2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mask(text):\n",
    "    encoded = t5_tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt')\n",
    "    input_ids = encoded['input_ids'].to(DEVICE)\n",
    "    \n",
    "    print(f\"INPUT TEXT: {text}\")\n",
    "\n",
    "    # Generaing 20 sequences with maximum length set to 5\n",
    "    outputs = t5_mlm.generate(input_ids=input_ids, \n",
    "                              num_beams=200, num_return_sequences=1, max_length=20)\n",
    "    \n",
    "    results = []\n",
    "    for output in outputs:\n",
    "        result = _filter(text, output)\n",
    "        results.append(result)\n",
    "\n",
    "def _filter(text, output, end_token='<extra_id_1>'):\n",
    "    # The first token is <unk> (inidex at 0) and the second token is <extra_id_0> (indexed at 32099)\n",
    "    _txt = t5_tokenizer.decode(output[2:], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    prev_end_token = \"<extra_id_0>\"\n",
    "    result = text[:text.index(prev_end_token)] # Set result to the substring before the first sentinel mask\n",
    "    for i in range(1,100):\n",
    "        end_token = \"<extra_id_\" + str(i) + \">\"\n",
    "        \n",
    "        print(f\"DEBUG: {text}\\n{prev_end_token}\")\n",
    "        _prev_index = text.index(prev_end_token)\n",
    "        _result_prefix = text[:_prev_index]\n",
    "        _result_suffix = text[_prev_index+len(prev_end_token):]\n",
    "        \n",
    "        print(f\"\\n\\nprefix: {_result_prefix}\\nsuffix: {_result_suffix}\")\n",
    "    \n",
    "        print(f\"TEST: {end_token}\\t{_txt}\")\n",
    "        if end_token in _txt:\n",
    "            print(f\"TOKEN {end_token} IS IN TEXT: {_txt}\\n{text}\")\n",
    "            _prev_end_token_index = _txt.index(prev_end_token) + len(prev_end_token)\n",
    "            _end_token_index = _txt.index(end_token)\n",
    "            if prev_end_token is None:\n",
    "                result = result + _txt[:_end_token_index]\n",
    "            else:\n",
    "                print(f\"STUFF TO ADD: {_txt[_prev_end_token_index:_end_token_index]}\")\n",
    "                result = result + _txt[_prev_end_token_index:_end_token_index]\n",
    "            #return _result_prefix + _txt[:_end_token_index] + _result_suffix\n",
    "        else:\n",
    "            print(f\"TOKEN {end_token} IS NOT IN TEXT\")\n",
    "            #return _result_prefix + _txt + _result_suffix\n",
    "            break\n",
    "        \n",
    "        print(f\"STATE OF RESULT: {result}\")\n",
    "        \n",
    "        prev_end_token = end_token\n",
    "        \n",
    "    print(f\"RESULT: {result + _result_suffix}\")\n",
    "    \n",
    "    return result + _result_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c39d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\">> User:\")\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    fill_mask(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c1f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8bf5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e56be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_masks_helper(text, output):\n",
    "    result = \"\"\n",
    "\n",
    "    starting_mask = \"<extra_id_0>\"\n",
    "    if starting_mask in text:\n",
    "        result = text[:text.index(starting_mask)]\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "    _txt = t5_tokenizer.decode(output, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    prev_mask = None\n",
    "    for i in range(0,100):\n",
    "        cur_mask = \"<extra_id_\" + str(i) + \">\"\n",
    "        next_mask = \"<extra_id_\" + str(i+1) + \">\"\n",
    "        \n",
    "        # Calculate the text to fill the mask\n",
    "        # If the model was not able to fill the current blank, then replace the mask with an empty string\n",
    "        #print(f\"Decoded text: {_txt}\")\n",
    "        if cur_mask in _txt:\n",
    "            _prev_end_token_index = _txt.index(cur_mask) + len(cur_mask)\n",
    "            if next_mask in _txt:\n",
    "                _end_token_index = _txt.index(next_mask)\n",
    "            else:\n",
    "                _end_token_index = len(_txt)\n",
    "            print(f\"Text to fill mask: {_txt[_prev_end_token_index:_end_token_index]}\")\n",
    "\n",
    "            # Fill the current mask\n",
    "            result += _txt[_prev_end_token_index:_end_token_index] #\"placeholder\"\n",
    "        \n",
    "        suffix_start_index = text.index(cur_mask) + len(cur_mask) # get the index right after the mask being replaced\n",
    "\n",
    "        if next_mask in text:\n",
    "            # There is a mask which hasn't been filled, append the string up till that mask\n",
    "            # The mask that has been found will be filled in the next loop\n",
    "            suffix_end_index = text.index(next_mask)\n",
    "        else:\n",
    "            # All the masks present in the original text have been replaced\n",
    "            # Append any remaining text after the last replaced mask and return the filled string\n",
    "            return result + text[suffix_start_index:]\n",
    "\n",
    "        # Append the result string with the substring after the mask that was filled in this loop\n",
    "        # Up till the next mask, of the end of the string, whichever comes first\n",
    "        result = result + text[suffix_start_index:suffix_end_index]\n",
    "        \n",
    "        # Update the previous mask\n",
    "        prev_mask = cur_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7781661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mask(text):\n",
    "    encoded = t5_tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt')\n",
    "    input_ids = encoded['input_ids'].to(DEVICE)\n",
    "    \n",
    "    print(f\"INPUT TEXT: {text}\")\n",
    "\n",
    "    # Generaing 20 sequences with maximum length set to 5\n",
    "    outputs = t5_mlm.generate(input_ids=input_ids, \n",
    "                              num_beams=10, num_return_sequences=3, max_length=6)\n",
    "    \n",
    "    results = []\n",
    "    for output in outputs:\n",
    "        result = fill_masks_helper(text, output)\n",
    "        print(f\"Result: {result}\")\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fcde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"How many tournaments has Pakistan won? <extra_id_0> six <extra_id_1> . </s>\"\n",
    "# text = \"why was the next edition of the tournament postponed to 2021?. It <extra_id_0> due to covid - 19 <extra_id_1> .\"\n",
    "killme = \"The World Drivers' Championship, which became the FIA Formula One World Championship in 1981, has been one of the premier forms of racing around the world since its inaugural season in 1950.\"\n",
    "# text = \"What is the capital of england and the united kingdom? London is the capital of the United Kingdom. <extra_id_0> london <extra_id_1> .\"\n",
    "killme1 = \"When did the Driver's Championship become the FIA Formula One World Championship?\"\n",
    "killme2 = \"<extra_id_0> 1981 <extra_id_1> .\"\n",
    "fill_mask(f\"{killme1} {killme2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8adebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_parsing = True\n",
    "cur_mask = \"<extra_id_0>\"\n",
    "\n",
    "# Calculate prefix index and prefix\n",
    "if cur_mask in text:\n",
    "    cur_prefix_index = text.index(cur_mask)\n",
    "    cur_prefix = text[:cur_prefix_index]\n",
    "else:\n",
    "    overall_suffix_start_index = text.index(prev_mask + len(prev_mask)) if prev_mask is not None else 0\n",
    "    result = result + text[overall_suffix_start_index:]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Calculate suffix indexes and suffix\n",
    "cur_suffix_start_index = cur_prefix_index + len(cur_mask)\n",
    "next_mask = \"<extra_id_1>\"\n",
    "if next_mask in text:\n",
    "    continue_parsing = True\n",
    "    cur_suffix_end_index = text.index(next_mask)\n",
    "else:\n",
    "    continue_parsing = False\n",
    "    cur_suffix_end_index = len(text)\n",
    "\n",
    "print(f\"suffix info: {cur_suffix_start_index}, {cur_suffix_end_index}\")\n",
    "cur_suffix = text[cur_suffix_start_index:cur_suffix_end_index]\n",
    "print(cur_prefix, \"\\n\", cur_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b9334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f9a73b1",
   "metadata": {},
   "source": [
    "<h3>OG FILL IN THE BLANKS CODE</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34207f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "\n",
    "T5_PATH = '../models/t5-base' # \"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # My envirnment uses CPU\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(T5_PATH)\n",
    "t5_config = T5Config.from_pretrained(T5_PATH)\n",
    "t5_mlm = T5ForConditionalGeneration.from_pretrained(T5_PATH, config=t5_config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae4cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "text = 'summarize: How many tournaments has Pakistan won? six <extra_id_0> . </s>'\n",
    "\n",
    "encoded = t5_tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt')\n",
    "input_ids = encoded['input_ids'].to(DEVICE)\n",
    "\n",
    "# Generaing 20 sequences with maximum length set to 5\n",
    "outputs = t5_mlm.generate(input_ids=input_ids, \n",
    "                          num_beams=200, num_return_sequences=5,\n",
    "                          max_length=5)\n",
    "\n",
    "_0_index = text.index('<extra_id_0>')\n",
    "_result_prefix = text[:_0_index]\n",
    "_result_suffix = text[_0_index+12:]  # 12 is the length of <extra_id_0>\n",
    "\n",
    "def _filter(output, end_token='<extra_id_1>'):\n",
    "    # The first token is <unk> (inidex at 0) and the second token is <extra_id_0> (indexed at 32099)\n",
    "    _txt = t5_tokenizer.decode(output[2:], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    if end_token in _txt:\n",
    "        _end_token_index = _txt.index(end_token)\n",
    "        return _result_prefix + _txt[:_end_token_index] + _result_suffix\n",
    "    else:\n",
    "        return _result_prefix + _txt + _result_suffix\n",
    "\n",
    "results = list(map(_filter, outputs))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555533fc",
   "metadata": {},
   "source": [
    "<h3>Question Generation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e6914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_path = \"../models/t5-base-e2e-qg\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c2ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q1 = \"\"\"summarize: Six tournaments have so far been played, and only the West Indies, who currently hold the title, has won the tournament on multiple occasions. The inaugural 2007 World Twenty20, was staged in South Africa, and won by India, who defeated Pakistan in the final at the Wanderers Stadium in Johannesburg. The 2009 tournament took place in England, and was won by the previous runner-up, Pakistan, who defeated Sri Lanka in the final at Lord's. The third tournament was held in 2010, hosted by the countries making up the West Indies cricket team.\"\"\"\n",
    "\n",
    "q2 = \"\"\"question: What does increased oxygen concentrations in the patient’s\n",
    "lungs displace? context: Hyperbaric (high-pressure) medicine uses special\n",
    "oxygen chambers to increase the partial pressure of O 2 around the patient\n",
    "and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene,\n",
    "and decompression sickness (the ’bends’) are sometimes treated using these\n",
    "devices. Increased O 2 concentration in the lungs helps to displace carbon\n",
    "monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the\n",
    "anaerobic bacteria that cause gas gangrene, so increasing its partial pressure\n",
    "helps kill them. Decompression sickness occurs in divers who decompress too\n",
    "quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and\n",
    "helium, forming in their blood. Increasing the pressure of O 2 as soon as\n",
    "possible is part of the treatment.\"\"\"\n",
    "\n",
    "input_ids = tokenizer(q1, return_tensors='pt', add_special_tokens=True).input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "for output in outputs:\n",
    "    print(tokenizer.decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c922dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "493ab524",
   "metadata": {},
   "source": [
    "<h3>T5 For Question Answering</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_path = \"../models/t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720644aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q0 = \"question: How many tournaments have been held so far? context: Six tournaments have so far been played, and only the West Indies, who currently hold the title, has won the tournament on multiple occasions. The inaugural 2007 World Twenty20, was staged in South Africa, and won by India, who defeated Pakistan in the final at the Wanderers Stadium in Johannesburg. The 2009 tournament took place in England, and was won by the previous runner-up, Pakistan, who defeated Sri Lanka in the final at Lord's. The third tournament was held in 2010, hosted by the countries making up the West Indies cricket team.\"\n",
    "\n",
    "#q = \"I like Mercedes.\"\n",
    "#c = \"\"\"Multan is a city and capital of Multan Division located in Punjab, Pakistan. Situated on the bank of the Chenab River, Multan is Pakistan's 7th largest city and is the major cultural and economic centre of Southern Punjab. Multan's history stretches deep into antiquity. The ancient city was site of the renowned Hindu Multan Sun Temple, and was besieged by Alexander the Great during the Mallian Campaign. Multan was one of the most important trading centres of medieval Islamic India, and attracted a multitude of Sufi mystics in the 11th and 12th centuries, earning the city the sobriquet \"City of Saints\". The city, along with the nearby city of Uch, is renowned for its large number of Sufi shrines dating from that era.\"\"\"\n",
    "\n",
    "q = \"Is Karachi liberal?\"\n",
    "c = \"Karachi is Pakistan's most cosmopolitan city, linguistically, ethnically, and religiously diverse, as well as one of Pakistan's most secular and socially liberal cities.\"\n",
    "question = f\"question: {q} context: {c}\"\n",
    "# q1 = \"ask_question: Six tournaments have so far been played, and only the West Indies, who currently hold the title, has won the tournament on multiple occasions. The inaugural 2007 World Twenty20, was staged in South Africa, and won by India, who defeated Pakistan in the final at the Wanderers Stadium in Johannesburg. The 2009 tournament took place in England, and was won by the previous runner-up, Pakistan, who defeated Sri Lanka in the final at Lord's. The third tournament was held in 2010, hosted by the countries making up the West Indies cricket team.\"\n",
    "# q2 = \"ask_question: Organised by cricket's governing body, the International Cricket Council (ICC), the tournament currently consists of 16 teams, comprising the top ten teams from the rankings at the given deadline and six other teams chosen through the T20 World Cup Qualifier.\"\n",
    "input_ids = tokenizer(question, return_tensors='pt', add_special_tokens=True).input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "decoded_outputs = []\n",
    "for output in outputs:\n",
    "    decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    print(decoded_output)\n",
    "    decoded_outputs.append(decoded_output)\n",
    "    \n",
    "\n",
    "# decoded_outputs[0] = \"Mumbai\"\n",
    "inp = f\"mnli hypothesis: {decoded_outputs[0]} . premise: {c} {q}\"\n",
    "\n",
    "input_ids = tokenizer(inp, return_tensors='pt', add_special_tokens=True).input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "for output in outputs:\n",
    "    print(tokenizer.decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a175400",
   "metadata": {},
   "source": [
    "<h4>No Answer Sanity Check</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3422132",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = f\"qnli question: {c} Which river is near Multan? The Chenab River . sentence: I also don't know what you're talking about\"\n",
    "\n",
    "input_ids = tokenizer(inp, return_tensors='pt', add_special_tokens=True).input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "for output in outputs:\n",
    "    print(tokenizer.decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e92db5",
   "metadata": {},
   "source": [
    "<h4>Contradiction Check - For QA</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d4cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = f\"mnli hypothesis: Daimler Benz . premise: Karl Benz founded Mercedes in 1901. Who founded Mercedes?\"\n",
    "\n",
    "input_ids = tokenizer(inp, return_tensors='pt', add_special_tokens=True).input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "for output in outputs:\n",
    "    print(tokenizer.decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5307416d",
   "metadata": {},
   "source": [
    "<h3>Contradiction Check - for answer fragments</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d945b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = f\"mnli hypothesis: Henry Ford. premise: Karl Benz founded Mercedes in 1901. Who founded Mercedes? Karl Benz\"\n",
    "\n",
    "input_ids = tokenizer(inp, return_tensors='pt', add_special_tokens=True).input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "for output in outputs:\n",
    "    print(tokenizer.decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa75e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "t20_text = \"Organised by cricket's governing body, the International Cricket Council (ICC), the tournament currently consists of 16 teams, comprising the top ten teams from the rankings at the given deadline and six other teams chosen through the T20 World Cup Qualifier\"\n",
    "inp = f\"summarize: {t20_text}. What do you think about the qualifiers?\"\n",
    "\n",
    "input_ids = tokenizer(inp, return_tensors='pt', add_special_tokens=True).input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "for output in outputs:\n",
    "    print(tokenizer.decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b47cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

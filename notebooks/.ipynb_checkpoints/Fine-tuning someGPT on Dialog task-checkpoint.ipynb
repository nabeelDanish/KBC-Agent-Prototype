{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e850d1e",
   "metadata": {},
   "source": [
    "<h1>Fine-tuning someGPT on Dialog task</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3965009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "cuda.empty_cache()\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "### For displaying\n",
    "\n",
    "import os\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "\n",
    "# define a rich console logger\n",
    "console=Console(record=True)\n",
    "\n",
    "def display_df(df):\n",
    "  \"\"\"display dataframe in ASCII format\"\"\"\n",
    "\n",
    "  console=Console()\n",
    "  table = Table(Column(\"source_text\", justify=\"center\" ), Column(\"target_text\", justify=\"center\"), title=\"Sample Data\",pad_edge=False, box=box.ASCII)\n",
    "\n",
    "  for i, row in enumerate(df.values.tolist()):\n",
    "    table.add_row(row[0], row[1])\n",
    "\n",
    "  console.print(table)\n",
    "\n",
    "training_logger = Table(Column(\"Epoch\", justify=\"center\" ), \n",
    "                        Column(\"Steps\", justify=\"center\"),\n",
    "                        Column(\"Loss\", justify=\"center\"), \n",
    "                        title=\"Training Status\",pad_edge=False, box=box.ASCII)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9750b7b6",
   "metadata": {},
   "source": [
    "<h3>Load dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a34bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               prev_turn  \\\n",
      "0                                                          \n",
      "1             I'm a huge fan of science fiction myself!    \n",
      "2      I agree. One of my favorite forms of science f...   \n",
      "3      And that's difficult to do when dealing with t...   \n",
      "4      Thank you for the suggestion, I will definitel...   \n",
      "...                                                  ...   \n",
      "82717  I love heavy metal music. My favorite bands ar...   \n",
      "82718  Awesome, it helps me relax and relieve my stre...   \n",
      "82719  It's a great form of exercise as well. I usual...   \n",
      "82720  Yeah, that would be awesome. At least Ozzy Osb...   \n",
      "82721  Yeah metal wouldn't be the same without Ozzy. ...   \n",
      "\n",
      "                                              human_sent  \\\n",
      "0      I think science fiction is an amazing genre fo...   \n",
      "1      Awesome! I really love how sci-fi storytellers...   \n",
      "2      It's not quite sci-fi, but my favorite version...   \n",
      "3      If you really want a look at the potential neg...   \n",
      "4      It blends science fiction and paranormal/psych...   \n",
      "...                                                  ...   \n",
      "82717  That's cool! I like heavy metal too, especiall...   \n",
      "82718  Ah yes, moshing is a staple at punk and heavy ...   \n",
      "82719  Sounds fun! I kinda wish I was around in the l...   \n",
      "82720  Lol oh yes. In fact, his longevity and success...   \n",
      "82721  Totally! What's crazy is that he was fired fro...   \n",
      "\n",
      "                                          knowledge_sent  \n",
      "0      Science fiction (often shortened to SF or sci-...  \n",
      "1      Science fiction films have often been used to ...  \n",
      "2      The central premise for these stories oftentim...  \n",
      "3      Science fiction often explores the potential c...  \n",
      "4                                       no_passages_used  \n",
      "...                                                  ...  \n",
      "82717  Heavy metal (or simply metal) is a genre of ro...  \n",
      "82718  Since then, moshing has occasionally been perf...  \n",
      "82719  In 1968, three of the genre's most famous acts...  \n",
      "82720  His longevity and success have earned him the ...  \n",
      "82721  He was fired from Black Sabbath in 1979 and we...  \n",
      "\n",
      "[82722 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('wizards-of-wikipedia-data-extraction/out.pkl')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd3e7a5",
   "metadata": {},
   "source": [
    "<h3>Model Initialization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99bd9362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 21:11:40.592965: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2021-12-01 21:11:40.592993: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: farjad-Lenovo-Y520\n",
      "2021-12-01 21:11:40.593012: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: farjad-Lenovo-Y520\n",
      "2021-12-01 21:11:40.593269: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.82.0\n",
      "2021-12-01 21:11:40.593291: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.82.0\n",
      "2021-12-01 21:11:40.593296: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.82.0\n",
      "2021-12-01 21:11:40.593716: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-01 21:11:40.682463: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "# loading tokenizer from the saved model path\n",
    "model_path = '../models/DialoGPT-small/saves'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "tokenizer.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})\n",
    "# creating the configurations from which the model can be made\n",
    "config = GPT2Config(\n",
    "  vocab_size=tokenizer.vocab_size,\n",
    "  bos_token_id=tokenizer.bos_token_id,\n",
    "  eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "# creating the model\n",
    "model = TFGPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db870090",
   "metadata": {},
   "source": [
    "<h3>Dataset Class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7badf449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and \n",
    "    loading it into the dataloader to pass it to the neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "\n",
    "        #cleaning data so as to ensure data is in string type\n",
    "        source_text = ' '.join(source_text.split())\n",
    "        target_text = ' '.join(target_text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        temp = {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd534fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8463e4f9",
   "metadata": {},
   "source": [
    "<h2>Model Implementation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ead66492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT(dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\" ):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    tokenizer.add_special_tokens({\n",
    "      \"eos_token\": \"</s>\",\n",
    "      \"bos_token\": \"<s>\",\n",
    "      \"unk_token\": \"<unk>\",\n",
    "      \"pad_token\": \"<pad>\",\n",
    "      \"mask_token\": \"<mask>\"\n",
    "    })\n",
    "    # creating the configurations from which the model can be made\n",
    "    config = GPT2Config(\n",
    "      vocab_size=tokenizer.vocab_size,\n",
    "      bos_token_id=tokenizer.bos_token_id,\n",
    "      eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    # creating the model\n",
    "    model = TFGPT2LMHeadModel(config)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    dataframe = dataframe[[source_text,target_text]]\n",
    "    display_df(dataframe.head(2))\n",
    "\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
    "    train_size = 0.8\n",
    "    train_dataset=dataframe.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
    "    val_dataset=dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "    inputs = tokenizer.batch_encode_plus(train_dataset[source_text], max_length= model_params[\"MAX_SOURCE_TEXT_LENGTH\"], pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "    #print(train_dataset[source_text][0])\n",
    "    #print(len(inputs['input_ids']))\n",
    "    #print(inputs)\n",
    "    labels = tokenizer.batch_encode_plus(train_dataset[target_text], max_length= model_params[\"MAX_TARGET_TEXT_LENGTH\"], pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "    #print(train_dataset[target_text][0])\n",
    "    #print(len(labels['input_ids']))\n",
    "    #print(labels)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "    #print(dataset)\n",
    "    \n",
    "    #return\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    #training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "    #val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "\n",
    "    \"\"\"\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "      'shuffle': True,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "    val_params = {\n",
    "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "      'shuffle': False,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "    \n",
    "    print(training_set[0:2])\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f48b5a",
   "metadata": {},
   "source": [
    "<h3>Run the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ad915b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                         Sample Data                                         </span>\n",
       "+-------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">                source_text                  </span>|<span style=\"font-weight: bold\">                 target_text                 </span>|\n",
       "|---------------------------------------------+---------------------------------------------|\n",
       "| Science fiction (often shortened to SF or   | I think science fiction is an amazing genre |\n",
       "| sci-fi) is a genre of speculative fiction,  |  for anything. Future science, technology,  |\n",
       "|typically dealing with imaginative concepts  |  time travel, FTL travel, they're all such  |\n",
       "| such as futuristic science and technology,  |            interesting concepts.            |\n",
       "|space travel, time travel, faster than light |                                             |\n",
       "|      travel, parallel universes, and        |                                             |\n",
       "|           extraterrestrial life.            |                                             |\n",
       "| Science fiction films have often been used  |      Awesome! I really love how sci-fi      |\n",
       "|to focus on political or social issues, and  |            storytellers focus on            |\n",
       "|  to explore philosophical issues like the   |  political/social/philosophical issues that |\n",
       "|              human condition.               |  would still be around even in the future.  |\n",
       "|                                             |            Makes them relatable.            |\n",
       "+-------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                         Sample Data                                         \u001b[0m\n",
       "+-------------------------------------------------------------------------------------------+\n",
       "|\u001b[1m                source_text                 \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                target_text                 \u001b[0m|\n",
       "|---------------------------------------------+---------------------------------------------|\n",
       "| Science fiction (often shortened to SF or   | I think science fiction is an amazing genre |\n",
       "| sci-fi) is a genre of speculative fiction,  |  for anything. Future science, technology,  |\n",
       "|typically dealing with imaginative concepts  |  time travel, FTL travel, they're all such  |\n",
       "| such as futuristic science and technology,  |            interesting concepts.            |\n",
       "|space travel, time travel, faster than light |                                             |\n",
       "|      travel, parallel universes, and        |                                             |\n",
       "|           extraterrestrial life.            |                                             |\n",
       "| Science fiction films have often been used  |      Awesome! I really love how sci-fi      |\n",
       "|to focus on political or social issues, and  |            storytellers focus on            |\n",
       "|  to explore philosophical issues like the   |  political/social/philosophical issues that |\n",
       "|              human condition.               |  would still be around even in the future.  |\n",
       "|                                             |            Makes them relatable.            |\n",
       "+-------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">FULL Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">82722</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "FULL Dataset: \u001b[1m(\u001b[0m\u001b[1;36m82722\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TRAIN Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">66178</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TRAIN Dataset: \u001b[1m(\u001b[0m\u001b[1;36m66178\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TEST Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16544</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TEST Dataset: \u001b[1m(\u001b[0m\u001b[1;36m16544\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autism is a developmental disorder characterized by troubles with social interaction and communication.\n",
      "66178\n",
      "{'input_ids': tensor([[16541,  1042,   318,  ..., 50259, 50259, 50259],\n",
      "        [   32,  1204, 14864,  ..., 50259, 50259, 50259],\n",
      "        [   32,  4356, 39834,  ..., 50259, 50259, 50259],\n",
      "        ...,\n",
      "        [   39, 30921, 20351,  ..., 50259, 50259, 50259],\n",
      "        [ 6653,   717,  1492,  ..., 50259, 50259, 50259],\n",
      "        [28566,  1895,   373,  ..., 50259, 50259, 50259]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "Oh my, I'm sure it's good to find out. I know Autism is a developmental disorder, causing issues with communication and interaction in social situations, but not much more about it. Have you treatment plans?\n",
      "66178\n",
      "{'input_ids': tensor([[ 5812,   616,    11,  ..., 50259, 50259, 50259],\n",
      "        [ 2215,   314,   373,  ..., 50259, 50259, 50259],\n",
      "        [   32,  4356, 39834,  ..., 50259, 50259, 50259],\n",
      "        ...,\n",
      "        [   40,  1053,  2982,  ..., 50259, 50259, 50259],\n",
      "        [ 5779,   262,   366,  ..., 50259, 50259, 50259],\n",
      "        [ 3666,  5230,  1895,  ..., 50259, 50259, 50259]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "<TensorSliceDataset shapes: ({input_ids: (512,), attention_mask: (512,)}, {input_ids: (512,), attention_mask: (512,)}), types: ({input_ids: tf.int64, attention_mask: tf.int64}, {input_ids: tf.int64, attention_mask: tf.int64})>\n"
     ]
    }
   ],
   "source": [
    "src_text = 'knowledge_sent'\n",
    "out_text = 'human_sent'\n",
    "model_params={\n",
    "    \"MODEL\":\"../models/DialoGPT-small/saves\",             # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\":1,          # training batch size\n",
    "    \"VALID_BATCH_SIZE\":1,          # validation batch size\n",
    "    \"TRAIN_EPOCHS\":3,              # number of training epochs\n",
    "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "    \"LEARNING_RATE\":2e-4,          # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\":512,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\":512,   # max length of target text\n",
    "    \"SEED\": 42                     # set seed for reproducibility \n",
    "}\n",
    "GPT(df, src_text, out_text, model_params, 'myGPT_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ac86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e232dc13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ab083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

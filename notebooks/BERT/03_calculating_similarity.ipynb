{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Similarity\n",
    "\n",
    "When calculating similarity between our transformer embedded vectors, we can use any of the *three* similarity metrics already covered.\n",
    "\n",
    "But first, let's create some embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Karachi\",\n",
    "    \"cosmopolitan\",\n",
    "    \"marketplace\",\n",
    "    \"city\",\n",
    "    \"buy\",\n",
    "    \"advertise\"\n",
    "]\n",
    "\n",
    "# thanks to https://randomwordgenerator.com/sentence.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('../../models/bert-base-cased-squad2')\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('../../models/bert-base-cased-squad2')\n",
    "\n",
    "# initialize dictionary that will contain tokenized sentences\n",
    "tokens = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "for sentence in sentences:\n",
    "    # tokenize sentence and append to dictionary lists\n",
    "    new_tokens = tokenizer.encode_plus(sentence, max_length=128, truncation=True,\n",
    "                                       padding='max_length', return_tensors='pt')\n",
    "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "\n",
    "# reformat list of tensors into single tensor\n",
    "tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process these tokens through our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['start_logits', 'end_logits', 'hidden_states'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**tokens, output_hidden_states=True)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dense vector representations of our `text` are contained within the `outputs` **'last_hidden_state'** tensor, which we access like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0228,  0.3132, -0.7590,  ...,  0.4847, -0.0601, -1.1486],\n",
       "         [ 0.2346, -0.6184,  0.4789,  ..., -0.3706,  0.4964, -1.1067],\n",
       "         [-0.4202,  0.1938,  0.3223,  ...,  0.4876,  0.5470, -1.6546],\n",
       "         ...,\n",
       "         [ 0.2705, -0.7980,  1.0575,  ...,  0.1251,  0.5117, -1.2836],\n",
       "         [ 0.2530, -0.8059,  1.0278,  ...,  0.0902,  0.5184, -1.3027],\n",
       "         [ 0.2663, -0.7753,  0.9917,  ...,  0.0626,  0.5782, -1.2521]],\n",
       "\n",
       "        [[ 0.6517, -0.4943,  0.4826,  ..., -0.5239,  1.0242,  0.4323],\n",
       "         [ 0.3876, -0.3390,  0.5466,  ..., -0.7689,  0.2167, -0.4809],\n",
       "         [ 0.0898,  0.0048,  1.3895,  ..., -0.6799,  0.6728, -0.6674],\n",
       "         ...,\n",
       "         [ 0.4997, -0.4367,  1.4845,  ..., -0.0255,  0.2526, -1.3542],\n",
       "         [ 0.5141, -0.4033,  1.4899,  ..., -0.0335,  0.3059, -1.3596],\n",
       "         [ 0.5047, -0.3955,  1.4584,  ..., -0.0819,  0.3089, -1.3398]],\n",
       "\n",
       "        [[ 0.0577, -0.1037,  1.3791,  ..., -0.7789,  0.5880, -0.0669],\n",
       "         [ 0.2101, -0.6814,  1.1493,  ..., -0.8293,  0.0084, -0.5248],\n",
       "         [-0.1273, -0.1992,  0.8594,  ...,  0.2336,  0.6366, -1.2243],\n",
       "         ...,\n",
       "         [-0.1956, -0.5736,  1.3326,  ..., -0.2438,  0.1631, -1.2498],\n",
       "         [-0.1945, -0.5695,  1.3315,  ..., -0.2131,  0.1739, -1.2825],\n",
       "         [-0.2174, -0.5479,  1.3292,  ..., -0.2290,  0.1707, -1.2811]],\n",
       "\n",
       "        [[-0.0549, -0.4056,  0.7345,  ..., -0.1780,  0.1861, -0.5753],\n",
       "         [ 0.2753, -1.3682,  0.6187,  ..., -0.6617,  0.1544, -1.0981],\n",
       "         [ 0.1766, -0.6127,  1.0118,  ...,  0.1698,  0.4160, -1.7294],\n",
       "         ...,\n",
       "         [-0.0967, -1.0680,  1.1949,  ..., -0.1752,  0.4415, -1.7848],\n",
       "         [-0.1176, -1.0673,  1.1813,  ..., -0.1698,  0.4730, -1.8301],\n",
       "         [-0.1321, -1.0504,  1.1176,  ..., -0.1406,  0.5312, -1.8334]],\n",
       "\n",
       "        [[ 0.1573,  0.4750,  0.5416,  ..., -0.7143,  0.7634, -0.2422],\n",
       "         [ 0.4426, -0.2806,  0.4399,  ..., -0.8528, -0.3648,  0.3362],\n",
       "         [-0.0933,  0.7357,  0.1604,  ..., -0.4844,  0.2784, -0.6184],\n",
       "         ...,\n",
       "         [-0.1001, -0.6883,  0.6827,  ...,  0.0417, -0.1205, -0.7010],\n",
       "         [-0.1302, -0.6476,  0.6024,  ...,  0.0537, -0.1375, -0.6693],\n",
       "         [-0.0778, -0.6272,  0.4718,  ...,  0.0421, -0.2013, -0.5843]],\n",
       "\n",
       "        [[ 0.0898,  0.4462,  1.4804,  ..., -0.3629,  0.5763, -0.5844],\n",
       "         [ 0.4593, -0.2290,  1.3496,  ..., -0.5899, -0.1482, -0.7820],\n",
       "         [ 0.1723,  0.3563,  1.1218,  ...,  0.1092, -0.3258, -1.4896],\n",
       "         ...,\n",
       "         [ 0.0572, -0.2194,  1.3069,  ..., -0.0273, -0.4239, -1.3827],\n",
       "         [ 0.0565, -0.2334,  1.3444,  ..., -0.0279, -0.4065, -1.3953],\n",
       "         [-0.0131, -0.2367,  1.3368,  ..., -0.0703, -0.4052, -1.4067]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = outputs.hidden_states[12]\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have produced our dense vectors `embeddings`, we need to perform a *mean pooling* operation on them to create a single vector encoding (the **sentence embedding**). To do this mean pooling operation we will need to multiply each value in our `embeddings` tensor by it's respective `attention_mask` value - so that we ignore non-real tokens.\n",
    "\n",
    "To perform this operation, we first resize our `attention_mask` tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = tokens['attention_mask']\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector above represents a single token attention mask - each token now has a vector of size 768 representing it's *attention_mask* status. Then we multiply the two tensors to apply the attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings = embeddings * mask\n",
    "masked_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0228,  0.3132, -0.7590,  ...,  0.4847, -0.0601, -1.1486],\n",
       "         [ 0.2346, -0.6184,  0.4789,  ..., -0.3706,  0.4964, -1.1067],\n",
       "         [-0.4202,  0.1938,  0.3223,  ...,  0.4876,  0.5470, -1.6546],\n",
       "         ...,\n",
       "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[ 0.6517, -0.4943,  0.4826,  ..., -0.5239,  1.0242,  0.4323],\n",
       "         [ 0.3876, -0.3390,  0.5466,  ..., -0.7689,  0.2167, -0.4809],\n",
       "         [ 0.0898,  0.0048,  1.3895,  ..., -0.6799,  0.6728, -0.6674],\n",
       "         ...,\n",
       "         [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[ 0.0577, -0.1037,  1.3791,  ..., -0.7789,  0.5880, -0.0669],\n",
       "         [ 0.2101, -0.6814,  1.1493,  ..., -0.8293,  0.0084, -0.5248],\n",
       "         [-0.1273, -0.1992,  0.8594,  ...,  0.2336,  0.6366, -1.2243],\n",
       "         ...,\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.0549, -0.4056,  0.7345,  ..., -0.1780,  0.1861, -0.5753],\n",
       "         [ 0.2753, -1.3682,  0.6187,  ..., -0.6617,  0.1544, -1.0981],\n",
       "         [ 0.1766, -0.6127,  1.0118,  ...,  0.1698,  0.4160, -1.7294],\n",
       "         ...,\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[ 0.1573,  0.4750,  0.5416,  ..., -0.7143,  0.7634, -0.2422],\n",
       "         [ 0.4426, -0.2806,  0.4399,  ..., -0.8528, -0.3648,  0.3362],\n",
       "         [-0.0933,  0.7357,  0.1604,  ..., -0.4844,  0.2784, -0.6184],\n",
       "         ...,\n",
       "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "        [[ 0.0898,  0.4462,  1.4804,  ..., -0.3629,  0.5763, -0.5844],\n",
       "         [ 0.4593, -0.2290,  1.3496,  ..., -0.5899, -0.1482, -0.7820],\n",
       "         [ 0.1723,  0.3563,  1.1218,  ...,  0.1092, -0.3258, -1.4896],\n",
       "         ...,\n",
       "         [ 0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we sum the remained of the embeddings along axis `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed = torch.sum(masked_embeddings, 1)\n",
    "summed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then sum the number of values that must be given attention in each position of the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "summed_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.,  ..., 3., 3., 3.],\n",
       "        [4., 4., 4.,  ..., 4., 4., 4.],\n",
       "        [3., 3., 3.,  ..., 3., 3., 3.],\n",
       "        [3., 3., 3.,  ..., 3., 3., 3.],\n",
       "        [3., 3., 3.,  ..., 3., 3., 3.],\n",
       "        [5., 5., 5.,  ..., 5., 5., 5.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate the mean as the sum of the embedding activations `summed` divided by the number of values that should be given attention in each position `summed_mask`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pooled = summed / summed_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0543, -0.0371,  0.0141,  ...,  0.2006,  0.3278, -1.3033],\n",
       "        [ 0.2214, -0.2275,  0.7728,  ..., -0.5768,  0.7151, -0.3456],\n",
       "        [ 0.0468, -0.3281,  1.1293,  ..., -0.4582,  0.4110, -0.6053],\n",
       "        [ 0.1323, -0.7955,  0.7883,  ..., -0.2233,  0.2522, -1.1343],\n",
       "        [ 0.1689,  0.3100,  0.3806,  ..., -0.6839,  0.2257, -0.1748],\n",
       "        [ 0.1389,  0.1010,  1.2612,  ..., -0.1814,  0.0524, -0.8526]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is how we calculate our dense similarity vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate cosine similarity for sentence `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from PyTorch tensor to numpy array\n",
    "mean_pooled = mean_pooled.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Karachi', 'cosmopolitan', 'marketplace', 'city', 'largest', 'advertise']\n"
     ]
    }
   ],
   "source": [
    "# calculate\n",
    "sim = cosine_similarity(\n",
    "    [mean_pooled[0]],\n",
    "    mean_pooled[1:]\n",
    ")\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6263751  0.59248245 0.6986703  0.536074   0.6119157 ]]\n"
     ]
    }
   ],
   "source": [
    "print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These similarities translate to:\n",
    "\n",
    "| Index | Sentence | Similarity |\n",
    "| --- | --- | --- |\n",
    "| 1 | \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\" | 0.3309 |\n",
    "| 2 | \"The person box was packed with jelly many dozens of months later.\" | 0.7219 |\n",
    "| 3 | \"Standing on one's head at job interviews forms a lasting impression.\" | 0.1748 |\n",
    "| 4 | \"It took him a month to finish the meal.\" | 0.4471 |\n",
    "| 5 | \"He found a leprechaun in his walnut shell.\" | 0.5548 |\n",
    "\n",
    "\n",
    "So, as intended, the most similar sentence is that in index **2** - which contains the same meaning as our first sentence, without using the same words:\n",
    "\n",
    "`\"Three years later, the coffin was still full of Jello.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

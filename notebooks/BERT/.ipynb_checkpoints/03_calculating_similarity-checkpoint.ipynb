{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Similarity\n",
    "\n",
    "When calculating similarity between our transformer embedded vectors, we can use any of the *three* similarity metrics already covered.\n",
    "\n",
    "But first, let's create some embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Three years later, the coffin was still full of Jello.\",\n",
    "    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
    "    \"The person box was packed with jelly many dozens of months later.\",\n",
    "    \"Standing on one's head at job interviews forms a lasting impression.\",\n",
    "    \"It took him a month to finish the meal.\",\n",
    "    \"He found a leprechaun in his walnut shell.\"\n",
    "]\n",
    "\n",
    "# thanks to https://randomwordgenerator.com/sentence.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../models/bert-base-cased-squad2 were not used when initializing BertModel: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('../../models/bert-base-cased-squad2')\n",
    "model = AutoModel.from_pretrained('../../models/bert-base-cased-squad2')\n",
    "\n",
    "# initialize dictionary that will contain tokenized sentences\n",
    "tokens = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "for sentence in sentences:\n",
    "    # tokenize sentence and append to dictionary lists\n",
    "    new_tokens = tokenizer.encode_plus(sentence, max_length=128, truncation=True,\n",
    "                                       padding='max_length', return_tensors='pt')\n",
    "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "\n",
    "# reformat list of tensors into single tensor\n",
    "tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process these tokens through our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**tokens)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dense vector representations of our `text` are contained within the `outputs` **'last_hidden_state'** tensor, which we access like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0316,  0.3213,  1.0839,  ..., -0.0315,  0.3153, -0.3369],\n",
       "         [-0.2773,  0.3218,  1.0824,  ..., -0.1640, -0.7640, -0.0356],\n",
       "         [ 0.3556, -0.1141,  1.7803,  ...,  0.4573,  0.0182, -1.3971],\n",
       "         ...,\n",
       "         [ 0.0284, -0.7136,  1.3945,  ...,  0.1442, -0.0084, -1.7079],\n",
       "         [ 0.0802, -0.6769,  1.3921,  ...,  0.1274,  0.0058, -1.6575],\n",
       "         [ 0.2010, -0.7192,  1.3515,  ...,  0.1734, -0.0769, -1.5365]],\n",
       "\n",
       "        [[-0.3922,  0.3952,  0.7426,  ..., -0.0303,  0.5689, -0.2515],\n",
       "         [-0.0624, -0.9280,  1.2167,  ...,  0.2665,  0.1439, -0.4486],\n",
       "         [ 0.2753, -0.4714,  1.0930,  ...,  0.2404,  1.1289, -0.4512],\n",
       "         ...,\n",
       "         [-0.0721, -0.7183,  1.3501,  ...,  0.1265, -0.2407, -1.4680],\n",
       "         [-0.0512, -0.6419,  1.3827,  ...,  0.0417, -0.1954, -1.5097],\n",
       "         [-0.0870, -0.6302,  1.3772,  ...,  0.0955, -0.1894, -1.5086]],\n",
       "\n",
       "        [[ 0.0136,  0.2703,  1.2669,  ..., -0.0415,  0.5272, -0.3850],\n",
       "         [-0.4467, -0.7345,  1.0505,  ...,  0.1835, -0.6769, -0.6844],\n",
       "         [-0.0593,  0.0634,  0.8688,  ..., -0.0120, -0.4066, -0.0595],\n",
       "         ...,\n",
       "         [-0.1161, -0.5893,  1.2412,  ..., -0.0552, -0.0445, -1.3718],\n",
       "         [-0.0885, -0.1157,  1.2464,  ..., -0.2015,  0.2511, -1.0816],\n",
       "         [ 0.0193,  0.0727,  1.1655,  ..., -0.3521,  0.3746, -0.9784]],\n",
       "\n",
       "        [[ 0.0230, -0.4365,  0.7005,  ..., -0.0568,  0.4352,  0.0456],\n",
       "         [-0.2900, -0.8995,  0.9732,  ...,  0.2564, -0.3143, -1.0177],\n",
       "         [-0.0583,  0.1029, -0.1213,  ..., -0.3084, -0.2553, -0.3964],\n",
       "         ...,\n",
       "         [-0.0806, -0.6165,  1.4421,  ...,  0.0164, -0.0747, -1.3127],\n",
       "         [-0.0382, -0.5795,  1.4448,  ...,  0.0192, -0.0588, -1.3303],\n",
       "         [ 0.0054, -0.6000,  1.3571,  ...,  0.0787, -0.0649, -1.3586]],\n",
       "\n",
       "        [[ 0.1696,  0.1333,  0.8192,  ...,  0.3176,  0.6411, -0.3282],\n",
       "         [-0.5741, -0.7043,  0.7593,  ..., -0.5658, -0.1546, -0.3549],\n",
       "         [ 0.0975, -0.7478,  0.8582,  ..., -0.1183, -0.5610, -0.3905],\n",
       "         ...,\n",
       "         [ 0.1569, -0.7609,  1.3856,  ...,  0.3469,  0.0519, -1.3698],\n",
       "         [ 0.1883, -0.7927,  1.4586,  ...,  0.3227,  0.0673, -1.3942],\n",
       "         [ 0.1828, -0.7907,  1.5600,  ...,  0.3091,  0.1059, -1.3433]],\n",
       "\n",
       "        [[ 0.5513,  0.1551,  1.1053,  ..., -0.1024,  0.5297, -0.6327],\n",
       "         [ 0.5246, -0.7600,  1.3392,  ..., -0.2752, -0.3019, -0.2013],\n",
       "         [ 0.3232,  0.0482,  1.3292,  ...,  0.3803,  0.1891, -1.2261],\n",
       "         ...,\n",
       "         [ 0.1585, -0.6869,  1.3638,  ..., -0.0434, -0.0551, -1.4802],\n",
       "         [ 0.1877, -0.6826,  1.3615,  ..., -0.0455,  0.0272, -1.4690],\n",
       "         [ 0.0787, -0.6720,  1.3166,  ..., -0.0352, -0.1777, -1.5106]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = outputs.last_hidden_state\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have produced our dense vectors `embeddings`, we need to perform a *mean pooling* operation on them to create a single vector encoding (the **sentence embedding**). To do this mean pooling operation we will need to multiply each value in our `embeddings` tensor by it's respective `attention_mask` value - so that we ignore non-real tokens.\n",
    "\n",
    "To perform this operation, we first resize our `attention_mask` tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = tokens['attention_mask']\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector above represents a single token attention mask - each token now has a vector of size 768 representing it's *attention_mask* status. Then we multiply the two tensors to apply the attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings = embeddings * mask\n",
    "masked_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0316,  0.3213,  1.0839,  ..., -0.0315,  0.3153, -0.3369],\n",
       "         [-0.2773,  0.3218,  1.0824,  ..., -0.1640, -0.7640, -0.0356],\n",
       "         [ 0.3556, -0.1141,  1.7803,  ...,  0.4573,  0.0182, -1.3971],\n",
       "         ...,\n",
       "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.3922,  0.3952,  0.7426,  ..., -0.0303,  0.5689, -0.2515],\n",
       "         [-0.0624, -0.9280,  1.2167,  ...,  0.2665,  0.1439, -0.4486],\n",
       "         [ 0.2753, -0.4714,  1.0930,  ...,  0.2404,  1.1289, -0.4512],\n",
       "         ...,\n",
       "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "        [[ 0.0136,  0.2703,  1.2669,  ..., -0.0415,  0.5272, -0.3850],\n",
       "         [-0.4467, -0.7345,  1.0505,  ...,  0.1835, -0.6769, -0.6844],\n",
       "         [-0.0593,  0.0634,  0.8688,  ..., -0.0120, -0.4066, -0.0595],\n",
       "         ...,\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[ 0.0230, -0.4365,  0.7005,  ..., -0.0568,  0.4352,  0.0456],\n",
       "         [-0.2900, -0.8995,  0.9732,  ...,  0.2564, -0.3143, -1.0177],\n",
       "         [-0.0583,  0.1029, -0.1213,  ..., -0.3084, -0.2553, -0.3964],\n",
       "         ...,\n",
       "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "        [[ 0.1696,  0.1333,  0.8192,  ...,  0.3176,  0.6411, -0.3282],\n",
       "         [-0.5741, -0.7043,  0.7593,  ..., -0.5658, -0.1546, -0.3549],\n",
       "         [ 0.0975, -0.7478,  0.8582,  ..., -0.1183, -0.5610, -0.3905],\n",
       "         ...,\n",
       "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[ 0.5513,  0.1551,  1.1053,  ..., -0.1024,  0.5297, -0.6327],\n",
       "         [ 0.5246, -0.7600,  1.3392,  ..., -0.2752, -0.3019, -0.2013],\n",
       "         [ 0.3232,  0.0482,  1.3292,  ...,  0.3803,  0.1891, -1.2261],\n",
       "         ...,\n",
       "         [ 0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we sum the remained of the embeddings along axis `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed = torch.sum(masked_embeddings, 1)\n",
    "summed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then sum the number of values that must be given attention in each position of the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "summed_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.,  ..., 15., 15., 15.],\n",
       "        [22., 22., 22.,  ..., 22., 22., 22.],\n",
       "        [16., 16., 16.,  ..., 16., 16., 16.],\n",
       "        [16., 16., 16.,  ..., 16., 16., 16.],\n",
       "        [12., 12., 12.,  ..., 12., 12., 12.],\n",
       "        [17., 17., 17.,  ..., 17., 17., 17.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate the mean as the sum of the embedding activations `summed` divided by the number of values that should be given attention in each position `summed_mask`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pooled = summed / summed_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0594, -0.2442,  1.1076,  ...,  0.3828, -0.0689, -0.7749],\n",
       "        [ 0.1026, -0.1534,  0.6878,  ...,  0.2588,  0.3581, -0.7802],\n",
       "        [-0.0840,  0.0138,  1.1218,  ...,  0.0100, -0.0371, -0.6318],\n",
       "        [-0.0884, -0.4308,  1.1361,  ..., -0.0542, -0.1450, -0.7998],\n",
       "        [ 0.0566, -0.4325,  0.7826,  ...,  0.2553,  0.0868, -0.4280],\n",
       "        [ 0.1879, -0.2350,  1.1156,  ...,  0.0360,  0.0325, -0.8093]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is how we calculate our dense similarity vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate cosine similarity for sentence `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.914305  , 0.93545866, 0.88925207, 0.87824136, 0.9339597 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from PyTorch tensor to numpy array\n",
    "mean_pooled = mean_pooled.detach().numpy()\n",
    "\n",
    "# calculate\n",
    "cosine_similarity(\n",
    "    [mean_pooled[0]],\n",
    "    mean_pooled[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These similarities translate to:\n",
    "\n",
    "| Index | Sentence | Similarity |\n",
    "| --- | --- | --- |\n",
    "| 1 | \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\" | 0.3309 |\n",
    "| 2 | \"The person box was packed with jelly many dozens of months later.\" | 0.7219 |\n",
    "| 3 | \"Standing on one's head at job interviews forms a lasting impression.\" | 0.1748 |\n",
    "| 4 | \"It took him a month to finish the meal.\" | 0.4471 |\n",
    "| 5 | \"He found a leprechaun in his walnut shell.\" | 0.5548 |\n",
    "\n",
    "\n",
    "So, as intended, the most similar sentence is that in index **2** - which contains the same meaning as our first sentence, without using the same words:\n",
    "\n",
    "`\"Three years later, the coffin was still full of Jello.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

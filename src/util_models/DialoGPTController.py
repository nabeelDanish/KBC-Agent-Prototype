"""
  MUKALMA - A Knowledge-Powered Conversational Agent
  Project Id: F21-20-R-KBCAgent

  DialoGPTController class - responsible for initializing a DialoGPT model
    - fine-tune a DialoGPT model on a dataset, loaded in a Pandas dataframe
    - predict method applies a DialoGPT model on sequence generation

  Partial Credits: https://github.com/Shivanandroy/T5-Finetuning-PyTorch
    - Used code for training and logging from notebook/T5_Fine_tuning_with_PyTorch.ipynb

  - Custom implementation of the predict() method in order to apply DialoGPT on a response completion task
  - DialoGPT must either generate a response given a question, and it can also be fed with a partial response to either
      complete it, or generate a follow-up response (like a 'double text')
"""

import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader
from torch import cuda

# Import the DialoGPT modules from huggingface's transformers
from transformers import AutoModelForCausalLM, AutoTokenizer
from .PTDataSetClass import PTDataSetClass

from rich.table import Column, Table
from rich import box
from rich.console import Console

# Logger, for pretty-printing a table
training_logger = Table(Column("Epoch", justify="center"),
                        Column("Steps", justify="center"),
                        Column("Loss", justify="center"),
                        title="Training Status", pad_edge=False, box=box.ASCII)

# Console logger
console = Console(record=True)


def display_df(df):
    """display dataframe in ASCII format"""
    df_console = Console()
    table = Table(Column("source_text", justify="center"), Column("target_text", justify="center"),
                  title="Sample Data",
                  pad_edge=False, box=box.ASCII)

    for i, row in enumerate(df.values.tolist()):
        table.add_row(row[0], row[1])

    df_console.print(table)


class DialoGPTController:
    def __init__(self, model_path, use_cuda=False):
        self.TAG = 'DialoGPTController'

        self.device = 'cuda' if use_cuda and cuda.is_available() else 'cpu'
        cuda.empty_cache()

        # Conversation hasn't started, so set turn and chat history to None
        self.turn = None
        self.chat_history_ids = None

        self.model_path = model_path

        # Initial values for model and tokenizer
        self.model = None
        self.tokenizer = None

    def initialize_model(self, refresh=False):
        # Loads model from disk into memory
        if self.tokenizer is None or refresh:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, local_files_only=True)
            self.tokenizer.add_special_tokens({'pad_token': self.tokenizer.eos_token})
        if self.model is None or refresh:
            console.log(f"""[{self.TAG}]: Loading {self.model_path}...\n""")
            self.model = AutoModelForCausalLM.from_pretrained(self.model_path, bos_token_id=self.tokenizer.bos_token_id,
                                                              eos_token_id=self.tokenizer.eos_token_id,
                                                              local_files_only=True)
            self.model = self.model.to(self.device)
            self.model.resize_token_embeddings(len(self.tokenizer))

    def predict(self, user_input, output_fragment="", new_dialog_session=False):
        """
        Custom implementation of DialoGPT's predict functionality. Given an input message (and optionally, a partial
        response, this method uses DialoGPT for sequence generation)
        :param user_input: Input dialog. Can be a question or statement.
        :param output_fragment: (Optional) partial response which will be completed by DialoGPT, instead of generating
        an entirely new response
        :param new_dialog_session: Flag to indicate whether the input marks the start of a new conversation
        :return: The complete response, generated by DialoGPT
        """
        # If this is the first turn, initialize the turn variable
        if new_dialog_session or self.turn is None:
            self.turn = 0

        # Ensure the model is initialized
        self.initialize_model()

        # encode the new user input, add the eos_token and return a tensor in Pytorch
        new_user_input_ids = self.tokenizer.encode(
            user_input + self.tokenizer.eos_token + output_fragment, return_tensors='pt')

        # append the new user input tokens to the chat history
        bot_input_ids = torch.cat([self.chat_history_ids,
                                   new_user_input_ids], dim=-1) if self.turn > 0 else new_user_input_ids

        # generated a response while limiting the total chat history to 1000 tokens,
        # print(
        #    f"\ninput ids len: {bot_input_ids.shape}\nchat hist id len: "
        #    f"{self.chat_history_ids.shape if self.turn > 0 else new_user_input_ids.shape}"
        #    f"\nuser input id len: {new_user_input_ids.shape}\n")
        # temperature=0.7, num_beams=100, repetition_penalty=1.3,
        bot_input_ids = bot_input_ids.to(self.device, dtype=torch.long)

        # Nabeel Danish: Changed the parameters for the model to provide safer dialogues (experimental)
        # temperature=0.775, max_new_tokens=30, no_repeat_ngram_size=3, top_p=1.05, num_return_sequences=3\

        # We define 2 sets of parameters to feed into the model's generate() function
        # based on whether DialoGPT is generating an entire response based on input
        # or if given an input message and a partial response, it is completing the response

        # We use different sets of parameters since we found that allowing DialoGPT to be more adventurous by raising
        # its temperature and using different decoding methods works better when it must generate an entire response
        # Whereas if a partial response has been fed, it acts as the seed for DialoGPT's transformer-based attention
        # mechanism and hence, it performs better when it is less adventurous.
        chat_history_ids = None
        if output_fragment != "":
            chat_history_ids = self.model.generate(bot_input_ids,
                                                   max_length=1000,
                                                   pad_token_id=self.tokenizer.eos_token_id,
                                                   temperature=0.9,
                                                   num_beams=20,
                                                   repetition_penalty=1.3,
                                                   num_return_sequences=2, use_cache=True,
                                                   no_repeat_ngram_size=8)
        else:
            chat_history_ids = self.model.generate(bot_input_ids, pad_token_id=self.tokenizer.eos_token_id,
                                                   top_k=50, top_p=0.95, do_sample=True,
                                                   num_return_sequences=2, use_cache=True,
                                                   no_repeat_ngram_size=8)

        # prompt = [self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)
        #          for g in bot_input_ids]
        # print(f"\nPROMPT: {prompt}\n{bot_input_ids}\n\n{chat_history_ids}")

        # print the decoded, generated output for this conversation turn
        # model may have generated multiple responses
        best_response = None
        max_length = -1
        responses = []
        for response_id, response in enumerate(chat_history_ids[:, bot_input_ids.shape[-1]:]):
            responses.append(self.tokenizer.decode(response, skip_special_tokens=True))

        return responses

    def __train_step(self, loader, optimizer, epoch):
        """
        Function to be called for training with the parameters passed from main function
        """

        self.model.train()
        for _, data in enumerate(loader, 0):
            y = data['target_ids'].to(self.device, dtype=torch.long)
            y_ids = y[:, :-1].contiguous()
            lm_labels = y[:, 1:].clone().detach()
            lm_labels[y[:, 1:] == self.tokenizer.pad_token_id] = -100
            ids = data['source_ids'].to(self.device, dtype=torch.long)
            mask = data['source_mask'].to(self.device, dtype=torch.long)

            outputs = self.model(input_ids=ids, attention_mask=mask, labels=y)
            loss = outputs[0]

            if _ % 10 == 0:
                training_logger.add_row(str(epoch), str(_), str(loss))
                console.print(training_logger)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    def train(self, train_batch_size, valid_batch_size, train_epochs, val_epochs, learning_rate,
              max_source_text_length, max_target_text_length, dataframe, source_text_key, target_text_key, output_dir):

        # Set random seeds and deterministic pytorch for reproducibility
        torch.manual_seed(42)  # pytorch random seed
        np.random.seed(42)  # numpy random seed
        torch.backends.cudnn.deterministic = True

        # Importing the raw dataset
        console.log(f"[{self.TAG}]: Reading data...\n")
        dataframe = dataframe[[source_text_key, target_text_key]]
        display_df(dataframe.head(2))

        # Creation of Dataset and Dataloader
        # Defining the train size. So 80% of the data will be used for training and the rest for validation.
        train_size = 0.8
        train_dataset = dataframe.sample(frac=train_size, random_state=42)
        val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)
        train_dataset = train_dataset.reset_index(drop=True)

        console.print(f"FULL Dataset: {dataframe.shape}")
        console.print(f"TRAIN Dataset: {train_dataset.shape}")
        console.print(f"TEST Dataset: {val_dataset.shape}\n")

        # Creating the Training and Validation dataset for further creation of Dataloader
        training_set = PTDataSetClass(train_dataset, self.tokenizer, max_source_text_length,
                                      max_target_text_length, source_text_key, target_text_key)
        val_set = PTDataSetClass(val_dataset, self.tokenizer, max_source_text_length,
                                 max_target_text_length, source_text_key, target_text_key)

        # Defining the parameters for creation of data loaders
        train_params = {
            'batch_size': train_batch_size,
            'shuffle': True,
            'num_workers': 0
        }

        val_params = {
            'batch_size': valid_batch_size,
            'shuffle': False,
            'num_workers': 0
        }

        # Creation of Dataloaders for testing and validation. This will be used down for training and validation
        # stage for the model.
        training_loader = DataLoader(training_set, **train_params)
        val_loader = DataLoader(val_set, **val_params)

        # Defining the optimizer that will be used to tune the weights of the network in the training session.
        optimizer = torch.optim.Adam(params=self.model.parameters(), lr=learning_rate)

        # Training loop
        console.log(f'[Initiating Fine Tuning]...\n')

        for epoch in range(train_epochs):
            self.__train_step(training_loader, optimizer, epoch)

        console.log(f"[Saving Model]...\n")
        # Saving the model after training
        path = os.path.join(output_dir, "model_files")
        self.model.save_pretrained(path)
        self.tokenizer.save_pretrained(path)

        # evaluating test dataset
        console.log(f"[Initiating Validation]...\n")
        for epoch in range(val_epochs):
            knowledge_sent, predictions, actuals = self.validate(epoch, val_loader)
            final_df = pd.DataFrame(
                {'Knowledge Sentence': knowledge_sent, 'Generated Text': predictions, 'Actual Text': actuals})
            final_df.to_csv(os.path.join(output_dir, 'predictions.csv'))

        console.save_text(os.path.join(output_dir, 'logs.txt'))

        console.log(f"[Validation Completed.]\n")
        console.print(f"""[Model] Model saved @ {os.path.join(output_dir, "model_files")}\n""")
        console.print(
            f"""[Validation] Generation on Validation data saved @ {os.path.join(output_dir, 'predictions.csv')}\n""")
        console.print(f"""[Logs] Logs saved @ {os.path.join(output_dir, 'logs.txt')}\n""")

    def validate(self, epoch, loader):
        """
        Function to evaluate model for predictions
        """

        self.model.eval()
        prompts = []
        predictions = []
        actuals = []
        with torch.no_grad():
            for _, data in enumerate(loader, 0):
                y = data['target_ids'].to(self.device, dtype=torch.long)
                ids = data['source_ids'].to(self.device, dtype=torch.long)
                mask = data['source_mask'].to(self.device, dtype=torch.long)

                generated_ids = self.model.generate(
                    input_ids=ids,
                    attention_mask=mask,
                    max_length=64,
                    num_beams=2,
                    repetition_penalty=2.5,
                    length_penalty=1.0,
                    early_stopping=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

                prompt = [self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in
                          ids]
                preds = [self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in
                         generated_ids]
                target = [self.tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in
                          y]
                if _ % 10 == 0:
                    console.print(f'Completed {_}')

                prompts.extend(prompt)
                predictions.extend(preds)
                actuals.extend(target)

                # print(f"Predictions: {predictions}\nActuals: {actuals}\nPrompt: {prompt}")
        return prompts, predictions, actuals
